{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import subprocess  # to run the TRNSYS simulation\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the Experience Replay Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, max_size=1e6):\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "\n",
    "    def add(self, transition):\n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = transition\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        batch_states, batch_next_states, batch_actions, batch_rewards = [], [], [], []\n",
    "        \n",
    "        for i in ind:\n",
    "            state, next_state, action, reward = self.storage[i]\n",
    "            batch_states.append(np.array(state, copy=False))\n",
    "            batch_next_states.append(np.array(next_state, copy=False))\n",
    "            batch_actions.append(np.array(action, copy=False))\n",
    "            batch_rewards.append(np.array(reward, copy=False))\n",
    "            \n",
    "        return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(batch_rewards).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a neural network for the actor model and a neural network for the actor target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer_1 = nn.Linear(state_dim, 400)\n",
    "        self.layer_2 = nn.Linear(400, 300)\n",
    "        self.layer_3 = nn.Linear(300, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer_1(x))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        x = self.max_action * torch.tanh(self.layer_3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a neural network for the critic model and a neural network for the critic target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.layer_2 = nn.Linear(400, 300)\n",
    "        self.layer_3 = nn.Linear(300, 1)\n",
    "\n",
    "\n",
    "    ## outputs Q function Q(x,u)\n",
    "    def forward(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "        print(xu.size())\n",
    "        print(x.size(), u.size())\n",
    "        x1 = F.relu(self.layer_1(xu))\n",
    "        x1 = F.relu(self.layer_2(x1))\n",
    "        x1 = self.layer_3(x1)\n",
    "        return x1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Whole DDPG Training Process into a class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "os.chdir(r\"C:\\Users\\achatter\\Desktop\\PhDResearch\\DIET\\Controller\")\n",
    "\n",
    "class DDPG(object):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "    # Learning Process for the DDPG Algorithm\n",
    "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2, noise_clip=0.5):\n",
    "\n",
    "        for it in range(iterations):\n",
    "\n",
    "            # Step 4: We sample a batch of transitions (s, s', a, r) from the memory\n",
    "            batch_states, batch_next_states, batch_actions, batch_rewards = replay_buffer.sample(batch_size)\n",
    "            state = torch.Tensor(batch_states).to(device)\n",
    "            next_state = torch.Tensor(batch_next_states).to(device)\n",
    "            action = torch.Tensor(batch_actions).to(device)\n",
    "            reward = torch.Tensor(batch_rewards).to(device)\n",
    "\n",
    "            # Step 5: From the next state s', the Actor target plays the next action a'\n",
    "            next_action = self.actor_target(next_state)\n",
    "            print(next_action.size())\n",
    "            break\n",
    "\n",
    "            # Step 6: We add Gaussian noise to this next action a' and we clamp it in a range of values supported by the environment\n",
    "            noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
    "            noise = noise.clamp(-noise_clip, noise_clip)\n",
    "            next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            # Step 7: The Critic Target take (s', a') as input and return Q-value Qt(s', a') as output\n",
    "            target_q = self.critic_target(next_state, next_action)\n",
    "\n",
    "            # Step 8: We get the estimated reward, which is: r' = r + γ * Qt, where γ id the discount factor\n",
    "            target_q = reward + (discount * target_q).detach()\n",
    "\n",
    "            # Step 9: The Critic models take (s, a) as input and return Q-value Q(s, a) as output\n",
    "            current_q = self.critic(state, action)\n",
    "\n",
    "            # Step 10: We compute the loss coming from the Critic model: Critic Loss = MSE_Loss(Q(s,a), Qt)\n",
    "            critic_loss = F.mse_loss(current_q, target_q)\n",
    "\n",
    "            # Step 11: We back propagate this Critic loss and update the parameters of the Critic model with a SGD optimizer\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            # Step 12: We update our Actor model by performing gradient ascent on the output of the first Critic model\n",
    "            actor_loss = -self.critic.forward(state, self.actor(state)).mean()\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # Step 13: We update the weights of the Actor target by polyak averaging\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "            # Step 14: We update the weights of the Critic target by polyak averaging\n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "                \n",
    "        print(\"#######################################\")\n",
    "        print(\"Model Trained after episode number: %f\" % sim_num)\n",
    "        print(\"#######################################\")\n",
    "\n",
    "    # Making a save method to save a trained model\n",
    "    def save(self, filename, directory):\n",
    "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "\n",
    "    # Making a load method to load a pre-trained model\n",
    "    def load(self, filename, directory):\n",
    "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We make a function that evaluates the policy by calculating its average reward over 3 episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_policy(eval_episodes=1):\n",
    "    avg_reward = 0.\n",
    "    for eval_sim in range(eval_episodes):\n",
    "        # We reset the state of the environment [Tair, RHair, Tmrt, Vair, Tout, Clo, Met, Occ, Qheat]\n",
    "        obs = np.array([20.0, 50.0, 20.0, 0.1, 5.5, 1.0, 1.0, 0.0001, 0.0001])\n",
    "\n",
    "        # Store the first observations in the text file\n",
    "        state_txt = open(r\"C:\\Users\\achatter\\Desktop\\PhDResearch\\DIET\\Controller\\py_state.dat\", \"w\")\n",
    "        state_txt.truncate(0)\n",
    "        state_txt.write('\\t' + str(obs[0]) + '\\t' + str(obs[1]) + '\\t' + str(obs[2]) + '\\t' + str(obs[3]) + '\\t' + str(obs[4]) + '\\t' + str(obs[5]) + '\\t' + str(obs[6]) + '\\t' + str(obs[7]) + '\\t' +\n",
    "                        str(obs[8]) + '\\n')\n",
    "\n",
    "        #state_txt.write(\"\".join(['\\t' + str(num) for num in obs]) + '\\n')\n",
    "\n",
    "        state_txt.close()\n",
    "\n",
    "        # Erase the data from the previous episodes\n",
    "        next_state_txt = open(r\"C:\\Users\\achatter\\Desktop\\PhDResearch\\DIET\\Controller\\py_next_state.dat\", \"w\")\n",
    "        next_state_txt.truncate(0)\n",
    "        next_state_txt.close()\n",
    "\n",
    "        action_txt = open(r\"C:\\Users\\achatter\\Desktop\\PhDResearch\\DIET\\Controller\\py_action.dat\", \"w\")\n",
    "        action_txt.truncate(0)\n",
    "        action_txt.close()\n",
    "\n",
    "        reward_txt = open(r\"C:\\Users\\achatter\\Desktop\\PhDResearch\\DIET\\Controller\\py_reward.dat\", \"w\")\n",
    "        reward_txt.truncate(0)\n",
    "        reward_txt.close()\n",
    "\n",
    "        pmv_txt = open(r\"C:\\Users\\achatter\\Desktop\\PhDResearch\\DIET\\Controller\\py_pmv.dat\", \"w\")\n",
    "        pmv_txt.truncate(0)\n",
    "        pmv_txt.close()\n",
    "\n",
    "        # Running TRNSYS simulation\n",
    "        subprocess.run([r\"C:\\TRNSYS18\\Exe\\TrnEXE64.exe\", r\"C:\\Users\\achatter\\Desktop\\PhDResearch\\DIET\\Controller\\BuildingModel_1day.dck\"])\n",
    "\n",
    "        # Reading the reward from text file and calculating the episode reward\n",
    "        reward_data = pd.read_csv(r\"C:\\Users\\achatter\\Desktop\\PhDResearch\\DIET\\Controller\\py_reward.dat\", sep=\"\\s+\", usecols=[0], names=[0], skiprows=2)\n",
    "        avg_reward += reward_data[0].sum()\n",
    "\n",
    "    avg_reward /= eval_episodes\n",
    "    print(\"********************************************\")\n",
    "    print(\"Average Reward over the Evaluation Step: %f\" % avg_reward)\n",
    "    print(\"********************************************\")\n",
    "    return avg_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We set the initial parameters of the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_sim = 2  # Number of simulations after which the reinforcement learning defines the actions\n",
    "max_sim = 10  # Total number of Trnsys simulations for training\n",
    "sim_num = 0\n",
    "save_models = True  # Boolean checker whether or not to save the pre-trained model\n",
    "expl_noise = 0.1  # Exploration noise - STD value of exploration Gaussian noise\n",
    "batch_size = 100  # Size of the batch\n",
    "discount = 0.99  # Discount factor gamma, used in the calculation of the total discounted reward\n",
    "tau = 0.005  # Target network update rate\n",
    "policy_noise = 0.2  # STD of Gaussian noise added to the actions for the exploration purposes\n",
    "noise_clip = 0.5  # Maximum value of the Gaussian noise added to the actions (policy)\n",
    "comfort_lim = 0.5  # Limiting value of comfort(+/-) on the PMV index\n",
    "alpha = 0.5  # Adjusting co-efficient for the comfort reward\n",
    "beta = 1 # Adjusting co-efficient for the energy reward\n",
    "obs = np.array([20.0, 50.0, 20.0, 0.1, 5.5, 1.0, 1.0, 0.0001, 0.0001])  # Initial state of the trnsys env"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We create a filename for the two saved models: the Actor and Critic Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Settings: DDPG_DIET\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "file_name = \"%s_%s\" % (\"DDPG\", \"DIET\")\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Settings: %s\" % file_name)\n",
    "print(\"---------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We get the necessary information on the states and actions in the chosen environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dim = 9\n",
    "action_dim = 1\n",
    "max_action = 21\n",
    "\n",
    "#TODO max action = 21 means we clip the next action between (-21,21) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We create the policy network (the Actor model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy = DDPG(state_dim, action_dim, max_action)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We create the Experience Replay memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We define a list where all the evaluation results over 3 episodes are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function taken from CBE comfort tool to calculate the pmv value for comfort evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def comfPMV(ta, tr, vel, rh, met, clo, wme=0):\n",
    "    pa = rh * 10 * math.exp(16.6536 - 4030.183 / (ta + 235))\n",
    "\n",
    "    icl = 0.155 * clo  # thermal insulation of the clothing in M2K/W\n",
    "    m = met * 58.15  # metabolic rate in W/M2\n",
    "    w = wme * 58.15  # external work in W/M2\n",
    "    mw = m - w  # internal heat production in the human body\n",
    "    if icl <= 0.078:\n",
    "        fcl = 1 + (1.29 * icl)\n",
    "    else:\n",
    "        fcl = 1.05 + (0.645 * icl)\n",
    "\n",
    "    # heat transfer coefficient by forced convection\n",
    "    hcf = 12.1 * math.sqrt(vel)\n",
    "    taa = ta + 273\n",
    "    tra = tr + 273\n",
    "    # we have verified that using the equation below or this tcla = taa + (35.5 - ta) / (3.5 * (6.45 * icl + .1)) does not affect the PMV value\n",
    "    tcla = taa + (35.5 - ta) / (3.5 * icl + 0.1)\n",
    "\n",
    "    p1 = icl * fcl\n",
    "    p2 = p1 * 3.96\n",
    "    p3 = p1 * 100\n",
    "    p4 = p1 * taa\n",
    "    p5 = (308.7 - 0.028 * mw) + (p2 * math.pow(tra / 100.0, 4))\n",
    "    xn = tcla / 100\n",
    "    xf = tcla / 50\n",
    "    eps = 0.00015\n",
    "\n",
    "    n = 0\n",
    "    while abs(xn - xf) > eps:\n",
    "        xf = (xf + xn) / 2\n",
    "        hcn = 2.38 * math.pow(abs(100.0 * xf - taa), 0.25)\n",
    "        if hcf > hcn:\n",
    "            hc = hcf\n",
    "        else:\n",
    "            hc = hcn\n",
    "        xn = (p5 + p4 * hc - p2 * math.pow(xf, 4)) / (100 + p3 * hc)\n",
    "        n += 1\n",
    "        if n > 150:\n",
    "            print('Max iterations exceeded')\n",
    "            return 1  # fixme should not return 1 but instead PMV=999 as per ashrae standard\n",
    "\n",
    "    tcl = 100 * xn - 273\n",
    "\n",
    "    # heat loss diff. through skin\n",
    "    hl1 = 3.05 * 0.001 * (5733 - (6.99 * mw) - pa)\n",
    "    # heat loss by sweating\n",
    "    if mw > 58.15:\n",
    "        hl2 = 0.42 * (mw - 58.15)\n",
    "    else:\n",
    "        hl2 = 0\n",
    "    # latent respiration heat loss\n",
    "    hl3 = 1.7 * 0.00001 * m * (5867 - pa)\n",
    "    # dry respiration heat loss\n",
    "    hl4 = 0.0014 * m * (34 - ta)\n",
    "    # heat loss by radiation\n",
    "    hl5 = 3.96 * fcl * (math.pow(xn, 4) - math.pow(tra / 100.0, 4))\n",
    "    # heat loss by convection\n",
    "    hl6 = fcl * hc * (tcl - ta)\n",
    "\n",
    "    ts = 0.303 * math.exp(-0.036 * m) + 0.028\n",
    "    pmv = ts * (mw - hl1 - hl2 - hl3 - hl4 - hl5 - hl6)\n",
    "    ppd = 100.0 - 95.0 * math.exp(-0.03353 * pow(pmv, 4.0) - 0.2179 * pow(pmv, 2.0))\n",
    "\n",
    "    return pmv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function called by Trnsys to get the next action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trnsys_sim(tair_in, rh_in, tmrt_in, vair_in, tout_in, clo_in, met_in, occ_in, qheat_in):\n",
    "    global obs\n",
    "    # Retrieve values from the parameters and inputs from Trnsys\n",
    "\n",
    "    # Calculate the values of the new actions\n",
    "    if sim_num < start_sim:\n",
    "        action = round(random.uniform(16, 21), 1)  # Choosing random values between 16 and 24 deg\n",
    "    else:  # After 2 episodes, we switch to the model\n",
    "        action_arr = policy.select_action(obs)\n",
    "        # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
    "        if expl_noise != 0:\n",
    "            action_arr = (action_arr + np.random.normal(0, expl_noise, size=1)).clip(16.0, 21.0)\n",
    "            action = action_arr[0]\n",
    "\n",
    "    # The agent performs the action in the environment, then reaches the next state and receives the reward\n",
    "    new_obs = np.array([tair_in, rh_in, tmrt_in, vair_in, tout_in, clo_in, met_in, occ_in, qheat_in])\n",
    "\n",
    "    pmv = comfPMV(tair_in, tmrt_in, vair_in, rh_in, met_in, clo_in)\n",
    "    \n",
    "    reward = beta * (1 - (qheat_in/15000)) + alpha * (1 - ((pmv + 0.5) ** 2)) * occ_in\n",
    "\n",
    "    obs = new_obs\n",
    "\n",
    "    # Step 6: Return the new values based on which action will be performed in Trnsys\n",
    "    return action, reward, pmv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that runs the training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_training():\n",
    "    global sim_num\n",
    "    action_data = pd.DataFrame()\n",
    "    reward_data = pd.DataFrame()\n",
    "\n",
    "    for ep_num in range(max_sim):\n",
    "\n",
    "        sim_num = ep_num\n",
    "        # If we are not at the very beginning, we start the training process of the model\n",
    "        if ep_num != 0:\n",
    "            print(\"Total simulations: {} Episode Num: {} Reward: {}\".format(action_data.size, ep_num, reward_data[0].sum()))\n",
    "            policy.train(replay_buffer, action_data.size, batch_size, discount, tau, policy_noise, noise_clip)\n",
    "            \n",
    "            if ep_num > 2:                \n",
    "                evaluations.append(evaluate_policy())\n",
    "                policy.save(file_name, directory=\"./pytorch_models/22022022\")  #CHANGE THE FOLDER NAME FOR SAVING THE MODEL\n",
    "                np.save(\"./results/22022022/%s\" % file_name, evaluations)                \n",
    "\n",
    "        # We reset the state of the environment [Tair, RHair, Tmrt, Vair, Tout, Clo, Met, Occ, Qheat]\n",
    "        obs = np.array([20.0, 50.0, 20.0, 0.1, 5.5, 1.0, 1.0, 0.0001, 0.0001])\n",
    "\n",
    "        # Store the first observations in the text file\n",
    "        state_txt = open(r\"C:\\Users\\achatter\\Desktop\\PhDResearch\\DIET\\Controller\\py_state.dat\", \"w\")\n",
    "        state_txt.truncate(0)\n",
    "        state_txt.write('\\t' + str(obs[0]) + '\\t' + str(obs[1]) + '\\t' + str(obs[2]) + '\\t' + str(obs[3]) + '\\t' +\n",
    "                        str(obs[4]) + '\\t' + str(obs[5]) + '\\t' + str(obs[6]) + '\\t' + str(obs[7]) + '\\t' +\n",
    "                        str(obs[8]) + '\\n')\n",
    "        state_txt.close()\n",
    "\n",
    "        # Erase the data from the previous episodes\n",
    "        next_state_txt = open(r\"C:\\Users\\achatter\\Desktop\\PhDResearch\\DIET\\Controller\\py_next_state.dat\", \"w\")\n",
    "        next_state_txt.truncate(0)\n",
    "        next_state_txt.close()\n",
    "\n",
    "        action_txt = open(r\"C:\\Users\\achatter\\Desktop\\PhDResearch\\DIET\\Controller\\py_action.dat\", \"w\")\n",
    "        action_txt.truncate(0)\n",
    "        action_txt.close()\n",
    "\n",
    "        reward_txt = open(r\"C:\\Users\\achatter\\Desktop\\PhDResearch\\DIET\\Controller\\py_reward.dat\", \"w\")\n",
    "        reward_txt.truncate(0)\n",
    "        reward_txt.close()\n",
    "\n",
    "        pmv_txt = open(r\"C:\\Users\\achatter\\Desktop\\PhDResearch\\DIET\\Controller\\py_pmv.dat\", \"w\")\n",
    "        pmv_txt.truncate(0)\n",
    "        pmv_txt.close()\n",
    "\n",
    "        # Running TRNSYS simulation\n",
    "        subprocess.run([r\"C:\\TRNSYS18\\Exe\\TrnEXE64.exe\", r\"C:\\Users\\achatter\\Desktop\\PhDResearch\\DIET\\Controller\\BuildingModel_2day.dck\"])\n",
    "\n",
    "        # Reading from the text files to fill the replay buffer\n",
    "        state_data = pd.read_csv(r\"C:\\Users\\achatter\\Desktop\\PhDResearch\\DIET\\Controller\\py_state.dat\", sep=\"\\s+\", usecols=[0, 1, 2, 3, 4, 5, 6, 7, 8], names=[0, 1, 2, 3, 4, 5, 6, 7, 8], skiprows=2, skipfooter=1)\n",
    "        next_state_data = pd.read_csv(r\"C:\\Users\\achatter\\Desktop\\PhDResearch\\DIET\\Controller\\py_next_state.dat\", sep=\"\\s+\", usecols=[0, 1, 2, 3, 4, 5, 6, 7, 8], names=[0, 1, 2, 3, 4, 5, 6, 7, 8], skiprows=2)\n",
    "        action_data = pd.read_csv(r\"C:\\Users\\achatter\\Desktop\\PhDResearch\\DIET\\Controller\\py_action.dat\", sep=\"\\s+\", usecols=[0], names=[0], skiprows=2)\n",
    "        reward_data = pd.read_csv(r\"C:\\Users\\achatter\\Desktop\\PhDResearch\\DIET\\Controller\\py_reward.dat\", sep=\"\\s+\", usecols=[0], names=[0], skiprows=2)\n",
    "        pmv_data = pd.read_csv(r\"C:\\Users\\achatter\\Desktop\\PhDResearch\\DIET\\Controller\\py_pmv.dat\", sep=\"\\s+\", usecols=[0], names=[0], skiprows=2)\n",
    "        \n",
    "         # Writing to .excel files to save the data from the episode\n",
    "        state_data.to_excel('C:/Users/achatter/Desktop/PhDResearch/DIET/Controller/Training_Data/22022022/Ep'+str(ep_num+1)+'/state.xlsx', index = False)  #CHANGE THE FOLDER NAME FOR SAVING THE MODEL\n",
    "        next_state_data.to_excel('C:/Users/achatter/Desktop/PhDResearch/DIET/Controller/Training_Data/22022022/Ep'+str(ep_num+1)+'/next_state.xlsx', index = False)  #CHANGE THE FOLDER NAME FOR SAVING THE MODEL\n",
    "        action_data.to_excel('C:/Users/achatter/Desktop/PhDResearch/DIET/Controller/Training_Data/22022022/Ep'+str(ep_num+1)+'/action.xlsx', index = False)  #CHANGE THE FOLDER NAME FOR SAVING THE MODEL\n",
    "        reward_data.to_excel('C:/Users/achatter/Desktop/PhDResearch/DIET/Controller/Training_Data/22022022/Ep'+str(ep_num+1)+'/reward.xlsx', index = False)  #CHANGE THE FOLDER NAME FOR SAVING THE MODEL\n",
    "        pmv_data.to_excel('C:/Users/achatter/Desktop/PhDResearch/DIET/Controller/Training_Data/22022022/Ep'+str(ep_num+1)+'/pmv.xlsx', index = False)  #CHANGE THE FOLDER NAME FOR SAVING THE MODEL\n",
    "\n",
    "        for ind in range(action_data.size):\n",
    "            obs = np.array([state_data[0][ind], state_data[1][ind], state_data[2][ind], state_data[3][ind], state_data[4][ind], state_data[5][ind], state_data[6][ind], state_data[7][ind], state_data[8][ind]])\n",
    "            new_obs = np.array([next_state_data[0][ind], next_state_data[1][ind], next_state_data[2][ind], next_state_data[3][ind], next_state_data[4][ind], next_state_data[5][ind],\n",
    "                     next_state_data[6][ind], next_state_data[7][ind], next_state_data[8][ind]])\n",
    "            action = np.array([action_data[0][ind]])\n",
    "            reward_arr = np.array([reward_data[0][ind]])\n",
    "\n",
    "            # Step 5: We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
    "            replay_buffer.add((obs, new_obs, action, reward_arr))\n",
    "\n",
    "    # We add the last policy evaluation to our list of evaluations and we save our model\n",
    "    avg_reward = evaluate_policy()\n",
    "    evaluations.append(avg_reward)\n",
    "    policy.save(file_name, directory=\"./pytorch_models/latest\")\n",
    "    np.save(\"./results/%s\" % file_name, evaluations)\n",
    "    print(\"********************************************\")\n",
    "    print(\"Average Reward over the Evaluation Step: %f\" % avg_reward)\n",
    "    print(\"********************************************\")\n",
    "    if save_models:\n",
    "        policy.save(\"%s\" % file_name, directory=\"./pytorch_models/22022022\")      #CHANGE THE FOLDER NAME FOR SAVING THE MODEL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\achatter\\AppData\\Roaming\\Python\\Python37\\site-packages\\ipykernel_launcher.py:51: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total simulations: 3840 Episode Num: 1 Reward: 4079.1439356297687\n",
      "torch.Size([100, 1])\n",
      "#######################################\n",
      "Model Trained after episode number: 1.000000\n",
      "#######################################\n",
      "Total simulations: 3922 Episode Num: 2 Reward: 4169.348051191405\n",
      "torch.Size([100, 1])\n",
      "#######################################\n",
      "Model Trained after episode number: 2.000000\n",
      "#######################################\n",
      "Total simulations: 3861 Episode Num: 3 Reward: 4176.424216405293\n",
      "torch.Size([100, 1])\n",
      "#######################################\n",
      "Model Trained after episode number: 3.000000\n",
      "#######################################\n",
      "********************************************\n",
      "Average Reward over the Evaluation Step: 1825.291578\n",
      "********************************************\n",
      "Total simulations: 3790 Episode Num: 4 Reward: 4103.332039726272\n",
      "torch.Size([100, 1])\n",
      "#######################################\n",
      "Model Trained after episode number: 4.000000\n",
      "#######################################\n",
      "********************************************\n",
      "Average Reward over the Evaluation Step: 2092.345937\n",
      "********************************************\n",
      "Total simulations: 3617 Episode Num: 5 Reward: 3850.7043491709983\n",
      "torch.Size([100, 1])\n",
      "#######################################\n",
      "Model Trained after episode number: 5.000000\n",
      "#######################################\n"
     ]
    }
   ],
   "source": [
    "run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc = trnsys_sim(20.0, 50.0, 20.0, 0.1, 5.5, 1.0, 1.0, 0.0001, 0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondafa8f368d65524f94bdb33a0d9561a17f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
