{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import envs\n",
    "from datetime import datetime\n",
    "import json\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import math\n",
    "from pyfmi import load_fmu\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim: int, size: int, batch_size: int = 32):\n",
    "        self.obs_dim = obs_dim\n",
    "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.max_size, self.batch_size = size, batch_size\n",
    "        self.ptr, self.size, = 0, 0\n",
    "\n",
    "    def store(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        act: np.ndarray, \n",
    "        rew: float, \n",
    "        next_obs: np.ndarray, \n",
    "        done: bool,\n",
    "    ):\n",
    "        self.obs_buf[self.ptr] = obs.reshape(self.obs_buf[self.ptr].shape)\n",
    "        self.next_obs_buf[self.ptr] = next_obs.reshape(self.obs_buf[self.ptr].shape)\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
    "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
    "        return dict(obs=self.obs_buf[idxs],\n",
    "                    next_obs=self.next_obs_buf[idxs],\n",
    "                    acts=self.acts_buf[idxs],\n",
    "                    rews=self.rews_buf[idxs],\n",
    "                    done=self.done_buf[idxs])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(128, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\Harold\\\\Desktop\\\\ENAC-Semester-Project\\\\DIET_Controller'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Logger:\n",
    "\n",
    "    def __init__(self):\n",
    "\n",
    "        ## get current time to set up logging directory\n",
    "        date = datetime.now()\n",
    "        temp = list([date.year,date.month,date.day,date.hour,date.minute])\n",
    "        temp = [str(x) for x in temp]\n",
    "        self.time = \"_\".join(temp)\n",
    "        self.RESULT_PATH = f\"{os.getcwd()}/results/{str(date.year)}_{str(date.month)}_{str(date.day)}/results_{self.time}\"\n",
    "\n",
    "\n",
    "        ## create directories\n",
    "        os.makedirs(self.RESULT_PATH,exist_ok=True)\n",
    "        os.makedirs(f\"{self.RESULT_PATH}/plots/summary\", exist_ok=True)\n",
    "        os.makedirs(f\"{self.RESULT_PATH}/plots/pmv_categories\", exist_ok=True)\n",
    "        os.makedirs(f\"{self.RESULT_PATH}/experiments_csv\", exist_ok=True)\n",
    "        os.makedirs(f\"{self.RESULT_PATH}/model_weights\", exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    def _plot(\n",
    "        self, \n",
    "        epsilons: List[float],\n",
    "        losses : List[float],\n",
    "        tair: List[float],\n",
    "        actions: List[float],\n",
    "        pmv : List[float],\n",
    "        qheat: List[float],\n",
    "        rewards: List[float],\n",
    "        occ : List[float],\n",
    "        plot_filename:str,\n",
    "        title:str\n",
    "       ):\n",
    "\n",
    "        epsilons = np.array(epsilons)\n",
    "        losses = np.array(losses)\n",
    "        tair= np.array(tair)\n",
    "        actions = np.array(actions)\n",
    "        pmv = np.array(pmv)\n",
    "        qheat = np.array(qheat)\n",
    "        rewards = np.array(rewards)\n",
    "        occ = np.array(occ)\n",
    "\n",
    "\n",
    "        # Plotting the summary of simulation\n",
    "        fig = make_subplots(rows=8, cols=1, shared_xaxes=True, vertical_spacing=0.02,\n",
    "                            specs=[[{\"secondary_y\": False}], [{\"secondary_y\": False}], [{\"secondary_y\": False}],\n",
    "                                   [{\"secondary_y\": True}], [{\"secondary_y\": True}], [{\"secondary_y\": False}],\n",
    "                                   [{\"secondary_y\": True}], [{\"secondary_y\": True}]])\n",
    "        \n",
    "        iterations = len(tair)\n",
    "        t = np.linspace(0.0, iterations -1, iterations)\n",
    "        # Add traces\n",
    "        fig.add_trace(go.Scatter(name='Tair(state)', x=t, y=tair.flatten(), mode='lines', line=dict(width=1, color='cyan')),\n",
    "                      row=1, col=1)\n",
    "        #fig.add_trace(go.Scatter(name='Tair_avg', x=t, y=pd.Series(tair.flatten()).rolling(window=60).mean(), mode='lines',\n",
    "        #              line=dict(width=2, color='blue')), row=1, col=1)\n",
    "\n",
    "        fig.add_trace(go.Scatter(name='Tset(action)', x=t, y=actions.flatten(), mode='lines', line=dict(width=1, color='fuchsia')),\n",
    "                      row=2, col=1)\n",
    "        fig.add_trace(go.Scatter(name='Tset_avg', x=t, y=pd.Series(actions.flatten()).rolling(window=24).mean(), mode='lines',\n",
    "                      line=dict(width=2, color='purple')), row=2, col=1)\n",
    "\n",
    "        fig.add_trace(go.Scatter(name='Pmv', x=t, y=pmv.flatten(), mode='lines', line=dict(width=1, color='gold')),\n",
    "                      row=3, col=1)\n",
    "        #fig.add_trace(go.Scatter(name='Pmv_avg', x=t, y=pd.Series(pmv.flatten()).rolling(window=60).mean(), mode='lines',\n",
    "        #              line=dict(width=2, color='darkorange')), row=3, col=1)\n",
    "\n",
    "        fig.add_trace(go.Scatter(name='Heating', x=t, y=qheat.flatten(), mode='lines', line=dict(width=1, color='red')),\n",
    "                      row=4, col=1, secondary_y=False)\n",
    "        fig.add_trace(go.Scatter(name='Heating_cumulative', x=t, y=np.cumsum(qheat.flatten()), mode='lines',\n",
    "                      line=dict(width=2, color='darkred')), row=4, col=1, secondary_y=True)\n",
    "\n",
    "        fig.add_trace(go.Scatter(name='Reward', x=t, y=rewards.flatten(), mode='lines', line=dict(width=1, color='lime')),\n",
    "                      row=5, col=1, secondary_y=False)\n",
    "        fig.add_trace(go.Scatter(name='Reward_cum', x=t, y=np.cumsum(rewards.flatten()), mode='lines',\n",
    "                      line=dict(width=2, color='darkgreen')), row=5, col=1, secondary_y=True)\n",
    "\n",
    "        fig.add_trace(go.Scatter(name='Occupancy', x=t, y=occ.flatten(), mode='lines',\n",
    "                      line=dict(width=1, color='black')), row=6, col=1)\n",
    "        ## training part\n",
    "\n",
    "        fig.add_trace(go.Scatter(name='Epsilons', x=t, y=epsilons.flatten(), mode='lines',\n",
    "                      line=dict(width=1, color='blue')), row=7, col=1)\n",
    "        fig.add_trace(go.Scatter(name='Training Loss', x=t, y=losses.flatten(), mode='lines',\n",
    "                      line=dict(width=1, color='darkblue')), row=8, col=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Set x-axis title\n",
    "        fig.update_xaxes(title_text=\"Timestep (-)\", row=6, col=1)\n",
    "        # Set y-axes titles\n",
    "        fig.update_yaxes(title_text=\"<b>Tair</b> (°C)\", range=[10, 24], row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"<b>Tset</b> (°C)\", range=[14, 22], row=2, col=1)\n",
    "        fig.update_yaxes(title_text=\"<b>PMV</b> (-)\", row=3, col=1)\n",
    "        fig.update_yaxes(title_text=\"<b>Heat Power</b> (kJ/hr)\", row=4, col=1, secondary_y=False)\n",
    "        fig.update_yaxes(title_text=\"<b>Heat Energy</b> (kJ)\", row=4, col=1, secondary_y=True)\n",
    "        fig.update_yaxes(title_text=\"<b>Reward</b> (-)\", row=5, col=1, range=[-5, 5], secondary_y=False)\n",
    "        fig.update_yaxes(title_text=\"<b>Tot Reward</b> (-)\", row=5, col=1, secondary_y=True)\n",
    "        fig.update_yaxes(title_text=\"<b>Occ</b> (-)\", row=6, col=1)\n",
    "        fig.update_yaxes(title_text=\"<b>Epsilon</b> (-)\", row=7, col=1)\n",
    "        fig.update_yaxes(title_text=\"<b>Loss</b> (-)\", row=8, col=1)\n",
    "\n",
    "\n",
    "        fig.update_xaxes(nticks=50)\n",
    "        fig.update_layout(template='plotly_white', font=dict(family=\"Courier New, monospace\", size=10),\n",
    "                          legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1, xanchor=\"right\", x=1))\n",
    "\n",
    "\n",
    "        fig.update_layout(title_text=title)\n",
    "            \n",
    "        pyo.plot(fig, filename=plot_filename)\n",
    "\n",
    "\n",
    "    def plot_pmv_percentages(self, pmv: np.ndarray, savepath:str, title:str):\n",
    "        \n",
    "        temp = pmv\n",
    "        intervals = []\n",
    "        \n",
    "        length = 8\n",
    "        lower= -2\n",
    "        step = 0.5\n",
    "        \n",
    "        ranges = np.zeros(length)\n",
    "        \n",
    "        for i in range(length):\n",
    "        \n",
    "            if i == 0:\n",
    "                ranges[i] = (temp < lower).sum()\n",
    "                interval = f\"[-inf,{lower}]\"\n",
    "                intervals.append(interval)\n",
    "        \n",
    "            elif i == 7:\n",
    "                upper = (i-1)*step + lower\n",
    "                ranges[i] = ( upper<= temp).sum()\n",
    "                interval = f\"[{upper},inf]\"\n",
    "                intervals.append(interval)\n",
    "        \n",
    "            else:\n",
    "                lower_1 = lower + (i-1)*step\n",
    "                upper_1 = lower + (i)*step\n",
    "                ranges[i] = (( lower_1 <= temp) &(temp < upper_1)).sum()\n",
    "                interval = f\"[{lower_1},{upper_1}]\"\n",
    "                intervals.append(interval)\n",
    "        \n",
    "        \n",
    "        ranges = ranges / ranges.sum()\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "        # assign data\n",
    "        data = pd.DataFrame({'intervals':intervals,\n",
    "                             'ranges': ranges\n",
    "                            })\n",
    "        \n",
    "        \n",
    "        # compute percentage of each format\n",
    "        percentage = []\n",
    "        for i in range(data.shape[0]):\n",
    "            pct = data.ranges[i] * 100\n",
    "            percentage.append(round(pct,2))\n",
    "        data['Percentage'] = percentage\n",
    "        \n",
    "    \n",
    "        f,a = plt.subplots(1,1, figsize=(15,7))\n",
    "        colors_list = ['darkred','coral', 'coral', 'seagreen', 'lime', 'seagreen','coral','darkred']\n",
    "    \n",
    "        graph = plt.bar(x= data.intervals,height= data.ranges, color = colors_list)\n",
    "    \n",
    "        plt.xlabel(\"PMV value interval\")\n",
    "        plt.ylabel(\"Percentage of hours in interval\")\n",
    "        plt.title(\"Number of hours the algorithm spent in different PMV intervals\")\n",
    "        \n",
    "        i = 0\n",
    "        for p in graph:\n",
    "            width = p.get_width()\n",
    "            height = p.get_height()\n",
    "            x, y = p.get_xy()\n",
    "            plt.text(x+width/2,\n",
    "                     y+height*1.01,\n",
    "                     str(data.Percentage[i])+'%',\n",
    "                     ha='center',\n",
    "                     weight='bold')\n",
    "            i+=1\n",
    "\n",
    "        plt.savefig(f\"{savepath}/{title}.png\",dpi=400)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    def plot_and_logging(self, episode_num, tair, actions, pmv, qheat, rewards, occ, losses,epsilons,agent):\n",
    "\n",
    "        plot_filename=f\"{self.RESULT_PATH}/plots/summary/summary_{episode_num+1}.html\"\n",
    "        self._plot(epsilons,losses,tair,actions,pmv,qheat,rewards,occ,plot_filename = plot_filename, title=f\"Episode Number {episode_num+1}\")\n",
    "        self.plot_pmv_percentages(pmv=np.array(pmv),savepath=f\"{self.RESULT_PATH}/plots/pmv_categories\", title = f\"PMV_Categories_{episode_num+1}\" )\n",
    "\n",
    "        ## padding losses and epsilons so that they fit into dataframe\n",
    "\n",
    "        len_difference = len(tair) - len(losses)\n",
    "        pad_losses = [0 for i in range(len_difference)]\n",
    "        pad_epsilon = [epsilons[0] for i in range(len_difference)]\n",
    "        losses = pad_losses + losses\n",
    "        epsilons = pad_epsilon + epsilons\n",
    "        data = pd.DataFrame({\"loss\": losses, \"epsilon\":epsilons, \"tair\":tair, \"action\":actions,\n",
    "        \"pmv\":pmv, \"qheat\":qheat,\"reward\":rewards, \"occ\":occ})\n",
    "        data.to_csv(f\"{self.RESULT_PATH}/experiments_csv/experiments_results_{episode_num+1}.csv\")\n",
    "\n",
    "        ## saving parameters of environment\n",
    "        \n",
    "        f = open(f\"{self.RESULT_PATH}/env_params_{self.time}.json\",\"w\")\n",
    "        f.write(json.dumps(env.log_dict(),indent=True))\n",
    "        f.close()\n",
    "        agent.save(directory=f\"{self.RESULT_PATH}/model_weights\",filename=f\"torch_ep_{episode_num + 1}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent interacting with environment.\n",
    "    \n",
    "    Attribute:\n",
    "        env : (Environment) custom Environment to interact with TRNSYS\n",
    "        memory (ReplayBuffer): replay memory to store transitions\n",
    "        batch_size (int): batch size for sampling\n",
    "        epsilon (float): parameter for epsilon greedy policy\n",
    "        epsilon_decay (float): step size to decrease epsilon\n",
    "        max_epsilon (float): max value of epsilon\n",
    "        min_epsilon (float): min value of epsilon\n",
    "        target_update (int): period for target model's hard update\n",
    "        gamma (float): discount factor\n",
    "        dqn (Network): model to train and select actions\n",
    "        dqn_target (Network): target model to update\n",
    "        optimizer (torch.optim): optimizer for training dqn\n",
    "        transition (list): transition information including \n",
    "                           state, action, reward, next_state, done\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        env: gym.Env,\n",
    "        memory_size: int,\n",
    "        batch_size: int,\n",
    "        target_update: int,\n",
    "        epsilon_decay: float,\n",
    "        max_epsilon: float = 1.0,\n",
    "        min_epsilon: float = 0.1,\n",
    "        gamma: float = 0.99,\n",
    "    ):\n",
    "        \"\"\"Initialization.\n",
    "        \n",
    "        Args:\n",
    "            env (gym.Env): custom Environment to interact with TRNSYS\n",
    "            memory_size (int): length of memory\n",
    "            batch_size (int): batch size for sampling\n",
    "            target_update (int): period for target model's hard update\n",
    "            epsilon_decay (float): step size to decrease epsilon\n",
    "            lr (float): learning rate\n",
    "            max_epsilon (float): max value of epsilon\n",
    "            min_epsilon (float): min value of epsilon\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        ## dimensions for the network\n",
    "        obs_dim = env.observation_dim\n",
    "        action_dim = env.action_dim\n",
    "        \n",
    "        self.env = env\n",
    "        self.memory = ReplayBuffer(obs_dim, memory_size, batch_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = max_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.max_epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.target_update = target_update\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # device: cpu / gpu\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "\n",
    "\n",
    "        print(self.device)\n",
    "\n",
    "        # networks: dqn, dqn_target\n",
    "        self.dqn = Network(obs_dim, action_dim).to(self.device)\n",
    "        self.dqn_target = Network(obs_dim, action_dim).to(self.device)\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "        self.dqn_target.eval()\n",
    "        \n",
    "        # optimizer\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters())\n",
    "\n",
    "        # transition to store in memory\n",
    "        self.transition = list()\n",
    "        \n",
    "        # mode: train / test\n",
    "        self.is_test = False\n",
    "\n",
    "\n",
    "    def __getattribute__(self, attr):\n",
    "        return object.__getattribute__(self, attr)\n",
    "\n",
    "    def __setattr__(self, attr, value):\n",
    "        object.__setattr__(self, attr, value)\n",
    "\n",
    "        \n",
    "\n",
    "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Select an action from the input state.\"\"\"\n",
    "        # epsilon greedy policy\n",
    "        if self.epsilon > np.random.random():\n",
    "            selected_action = np.random.choice(self.env.action_dim,1)[0]\n",
    "        else:\n",
    "            selected_action = self.dqn(\n",
    "                torch.FloatTensor(state.T).to(self.device)\n",
    "            ).argmax()\n",
    "            selected_action = selected_action.detach().cpu().numpy()\n",
    "        \n",
    "        if not self.is_test:\n",
    "            self.transition = [state, selected_action]\n",
    "        \n",
    "        return selected_action\n",
    "\n",
    "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n",
    "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "\n",
    "        if not self.is_test:\n",
    "            self.transition += [reward, next_state, done]\n",
    "            self.memory.store(*self.transition)\n",
    "    \n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def update_model(self) -> torch.Tensor:\n",
    "        \"\"\"Update the model by gradient descent.\"\"\"\n",
    "        samples = self.memory.sample_batch()\n",
    "\n",
    "        loss = self._compute_dqn_loss(samples)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "        \n",
    "    def train(self,num_iterations= None, num_episodes= 1):\n",
    "        \"\"\"Train the agent.\"\"\"\n",
    "        self.is_test = False\n",
    "        ## instantiate logger\n",
    "        logger = Logger()\n",
    "\n",
    "        for episode_num in range(num_episodes):\n",
    "        \n",
    "            state = self.env.reset()                \n",
    "\n",
    "            update_cnt = 0\n",
    "            epsilons = []\n",
    "            losses = []\n",
    "            tair = []\n",
    "            actions = []\n",
    "            pmv = []\n",
    "            qheat = []\n",
    "            rewards = []\n",
    "            occ = []\n",
    "\n",
    "            if num_iterations > self.env.numsteps:\n",
    "                print(f\"WARNING: Number of iterations chosen ({num_iterations})is higher than the number of steps of the environment ({self.env.numsteps}) \")\n",
    "                num_iterations=self.env.numsteps\n",
    "\n",
    "            if num_iterations is None:\n",
    "                num_iterations=self.env.numsteps\n",
    "\n",
    "\n",
    "            for i in range(num_iterations):\n",
    "\n",
    "                action = self.select_action(state)\n",
    "                next_state, reward, done,info = self.step(action)\n",
    "\n",
    "                if i % 100 == 0:\n",
    "                    print(f\"Iteration{i}\")\n",
    "\n",
    "                ## keeping track of the value we've seen\n",
    "                rewards.append(reward)\n",
    "                actions.append(env.action_to_temp[action])\n",
    "                pmv.append(info['pmv'][0])\n",
    "                d = env.observation_to_dict(next_state)\n",
    "                tair.append(d[\"Tair\"][0])\n",
    "                qheat.append(d[\"Qheat\"][0])\n",
    "                occ.append(d[\"Occ\"][0])\n",
    "\n",
    "                state = next_state\n",
    "\n",
    "                # if episode ends\n",
    "                #if done:\n",
    "                #    state = self.env.reset()\n",
    "\n",
    "                # if training is ready\n",
    "                if len(self.memory) >= self.batch_size:\n",
    "                    loss = self.update_model()\n",
    "                    losses.append(loss)\n",
    "                    update_cnt += 1\n",
    "\n",
    "                    # linearly decrease epsilon\n",
    "                    self.epsilon = max(\n",
    "                        self.min_epsilon, self.epsilon - (\n",
    "                            self.max_epsilon - self.min_epsilon\n",
    "                        ) * self.epsilon_decay\n",
    "                    )\n",
    "                    epsilons.append(self.epsilon)\n",
    "\n",
    "                    # if hard update is needed\n",
    "                    if update_cnt % self.target_update == 0:\n",
    "                        self._target_hard_update()\n",
    "\n",
    "\n",
    "            logger.plot_and_logging(episode_num, tair, actions, pmv, qheat, rewards, occ,losses, epsilons,self)\n",
    "\n",
    "\n",
    "\n",
    "        #self.env.close()\n",
    "                \n",
    "\n",
    "    def _compute_dqn_loss(self, samples: Dict[str, np.ndarray]) -> torch.Tensor:\n",
    "        \"\"\"Return dqn loss.\"\"\"\n",
    "        device = self.device  # for shortening the following lines\n",
    "        state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
    "        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
    "        action = torch.LongTensor(samples[\"acts\"].reshape(-1, 1)).to(device)\n",
    "        reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
    "        done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
    "\n",
    "        # G_t   = r + gamma * v(s_{t+1})  if state != Terminal\n",
    "        #       = r                       otherwise\n",
    "        curr_q_value = self.dqn(state).gather(1, action)\n",
    "        next_q_value = self.dqn_target(\n",
    "            next_state\n",
    "        ).max(dim=1, keepdim=True)[0].detach()\n",
    "        mask = 1 - done\n",
    "        target = (reward + self.gamma * next_q_value * mask).to(self.device)\n",
    "\n",
    "        # calculate dqn loss\n",
    "        loss = F.smooth_l1_loss(curr_q_value, target)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _target_hard_update(self):\n",
    "        \"\"\"Hard update: target <- local.\"\"\"\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "\n",
    "\n",
    "    # Making a save method to save a trained model\n",
    "    def save(self, filename, directory):\n",
    "        torch.save(self.dqn.state_dict(), '%s/%s_dqn.pth' % (directory, filename))\n",
    "        torch.save(self.dqn_target.state_dict(), '%s/%s_dqn_target.pth' % (directory, filename))\n",
    "\n",
    "    # Making a load method to load a pre-trained model\n",
    "    def load(self, filename, directory):\n",
    "        self.dqn.load_state_dict(torch.load('%s/%s_dqn.pth' % (directory, filename)))\n",
    "        self.dqn_target.load_state_dict(torch.load('%s/%s_dqn_target.pth' % (directory, filename)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 778\n",
    "\n",
    "\n",
    "def seed_torch(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "np.random.seed(seed)\n",
    "seed_torch(seed)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set environment parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('EnergyPlusEnv-v0')\n",
    "env.seed(seed)\n",
    "\n",
    "env.beta =1 \n",
    "env.alpha = 1\n",
    "env.min_temp = 16\n",
    "env.max_temp = 21\n",
    "env.action_dim =200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('EnergyPlusEnv-v0')\n",
    "env.seed(seed)\n",
    "\n",
    "env.beta =1 \n",
    "env.alpha = 1\n",
    "env.min_temp = 16\n",
    "env.max_temp = 21\n",
    "env.action_dim =200\n",
    "\n",
    "env.modelname = 'CELLS_v1.fmu'\n",
    "env.simulation_path = r'C:\\Users\\Harold\\Desktop\\ENAC-Semester-Project\\DIET_Controller\\custom_gym\\Eplus_simulation'\n",
    "env.param_list = ['Tair', 'RH', 'Tmrt', 'Tout', 'Qheat', 'Occ']\n",
    "\n",
    "env.days = 151,  \n",
    "env.hours = 24,  \n",
    "env.minutes = 60,\n",
    "env.seconds = 60,\n",
    "env.ep_timestep = 6\n",
    "\n",
    "\n",
    "memory_size = 1000\n",
    "batch_size = 32\n",
    "target_update = 100\n",
    "epsilon_decay = 1 / 2000\n",
    "\n",
    "agent = DQNAgent(env,memory_size,batch_size,target_update,epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration0\n",
      "Iteration100\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1080x504 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "agent.train(num_episodes=1, num_iterations=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyfmi import load_fmu\n",
    "#import numpy as np\n",
    "#os.chdir(r'C:\\Users\\Harold\\Desktop\\test_fmu')\n",
    "#model = load_fmu(\"CELLS_v1.fmu\",log_level=4)\n",
    "#\n",
    "#\n",
    "#simtime = 0\n",
    "#days = 151 \n",
    "#hours = 24  \n",
    "#minutes = 6\n",
    "#seconds = 60\n",
    "#ep_timestep = 6\n",
    "#\n",
    "#numsteps = days * hours * ep_timestep       # total number of simulation steps during the simulationx\n",
    "#timestop = days * hours * minutes * seconds # total time length of our simulation\n",
    "#secondstep = timestop / numsteps  # length of a single step in seconds\n",
    "#simtime = 0                                # keeps track of current time in the simulation\n",
    "#\n",
    "#opts = model.simulate_options()  # Get the default options\n",
    "#opts['ncp'] = numsteps  # Specifies the number of timesteps\n",
    "#opts['initialize'] = False\n",
    "#\n",
    "#\n",
    "##model.initialize(simtime, timestop)\n",
    "#model.initialize(start_time = simtime, stop_time_defined = True, stop_time = 86400)\n",
    "#curr_obs = np.array(list(model.get(['Tair', 'RH', 'Tmrt', 'Tout', 'Qheat', 'Occ'])))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e78a7ef29a5e3028f948eff69c34ba1d8ebd35a887497a02775c6aab840f6bc2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
