{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from typing import Dict, List, Tuple\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import gym\n",
    "import envs\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "\n",
    "import math\n",
    "from pyfmi import load_fmu\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "##tair = []\n",
    "##action = []\n",
    "##pmv = []\n",
    "##qheat = []\n",
    "##reward = []\n",
    "##occ = []\n",
    "##iterations = 10000\n",
    "##\n",
    "##for i in range(iterations):\n",
    "##    action_ = random.randint(0,env.action_dim -1 )\n",
    "##    obs, reward_, done, info = env.step(action=action_)\n",
    "##    reward.append(reward_)\n",
    "##    action.append(env.action_to_temp[action_])\n",
    "##    pmv.append(info['pmv'][0])\n",
    "##    d = env.observation_to_dict(obs)\n",
    "##    tair.append(d[\"tair_in\"][0])\n",
    "##    qheat.append(d[\"qheat_in\"][0])\n",
    "##    occ.append(d[\"occ_in\"][0])\n",
    "##\n",
    "##\n",
    "##tair = np.array(tair)\n",
    "##action = np.array(action)\n",
    "##pmv = np.array(pmv)\n",
    "##qheat = np.array(qheat)\n",
    "##reward = np.array(reward)\n",
    "##occ = np.array(occ)\n",
    "##\n",
    "##\n",
    "##t = np.linspace(0.0, iterations -1, iterations)\n",
    "##\n",
    "### Plotting the summary of simulation\n",
    "##fig = make_subplots(rows=6, cols=1, shared_xaxes=True, vertical_spacing=0.04,\n",
    "##                    specs=[[{\"secondary_y\": False}], [{\"secondary_y\": False}], [{\"secondary_y\": False}],\n",
    "##                           [{\"secondary_y\": True}], [{\"secondary_y\": True}], [{\"secondary_y\": False}]])\n",
    "### Add traces\n",
    "##fig.add_trace(go.Scatter(name='Tair(state)', x=t, y=tair.flatten(), mode='lines', line=dict(width=1, color='cyan')),\n",
    "##              row=1, col=1)\n",
    "##fig.add_trace(go.Scatter(name='Tair_avg', x=t, y=pd.Series(tair.flatten()).rolling(window=24).mean(), mode='lines',\n",
    "##              line=dict(width=2, color='blue')), row=1, col=1)\n",
    "##fig.add_trace(go.Scatter(name='Tset(action)', x=t, y=action.flatten(), mode='lines', line=dict(width=1, color='fuchsia')),\n",
    "##              row=2, col=1)\n",
    "##fig.add_trace(go.Scatter(name='Tset_avg', x=t, y=pd.Series(action.flatten()).rolling(window=24).mean(), mode='lines',\n",
    "##              line=dict(width=2, color='purple')), row=2, col=1)\n",
    "##fig.add_trace(go.Scatter(name='Pmv', x=t, y=pmv.flatten(), mode='lines', line=dict(width=1, color='gold')),\n",
    "##              row=3, col=1)\n",
    "##fig.add_trace(go.Scatter(name='Pmv_avg', x=t, y=pd.Series(pmv.flatten()).rolling(window=24).mean(), mode='lines',\n",
    "##              line=dict(width=2, color='darkorange')), row=3, col=1)\n",
    "##fig.add_trace(go.Scatter(name='Heating', x=t, y=qheat.flatten(), mode='lines', line=dict(width=1, color='red')),\n",
    "##              row=4, col=1, secondary_y=False)\n",
    "##fig.add_trace(go.Scatter(name='Heating_cumulative', x=t, y=np.cumsum(qheat.flatten()), mode='lines',\n",
    "##              line=dict(width=2, color='darkred')), row=4, col=1, secondary_y=True)\n",
    "##fig.add_trace(go.Scatter(name='Reward', x=t, y=reward.flatten(), mode='lines', line=dict(width=1, color='lime')),\n",
    "##              row=5, col=1, secondary_y=False)\n",
    "##fig.add_trace(go.Scatter(name='Reward_cum', x=t, y=np.cumsum(reward.flatten()), mode='lines',\n",
    "##              line=dict(width=2, color='darkgreen')), row=5, col=1, secondary_y=True)\n",
    "##fig.add_trace(go.Scatter(name='Occupancy', x=t, y=occ.flatten(), mode='lines',\n",
    "##              line=dict(width=1, color='black')), row=6, col=1)\n",
    "### Set x-axis title\n",
    "##fig.update_xaxes(title_text=\"Timestep (-)\", row=6, col=1)\n",
    "### Set y-axes titles\n",
    "##fig.update_yaxes(title_text=\"<b>Tair</b> (째C)\", range=[10, 24], row=1, col=1)\n",
    "##fig.update_yaxes(title_text=\"<b>Tset</b> (째C)\", range=[12, 30], row=2, col=1)\n",
    "##fig.update_yaxes(title_text=\"<b>PMV</b> (-)\", row=3, col=1)\n",
    "##fig.update_yaxes(title_text=\"<b>Heat Power</b> (kJ/hr)\", row=4, col=1, secondary_y=False)\n",
    "##fig.update_yaxes(title_text=\"<b>Heat Energy</b> (kJ)\", row=4, col=1, secondary_y=True)\n",
    "##fig.update_yaxes(title_text=\"<b>Reward</b> (-)\", row=5, col=1, range=[-5, 5], secondary_y=False)\n",
    "##fig.update_yaxes(title_text=\"<b>Tot Reward</b> (-)\", row=5, col=1, secondary_y=True)\n",
    "##fig.update_yaxes(title_text=\"<b>Fraction</b> (-)\", row=6, col=1)\n",
    "##fig.update_xaxes(nticks=50)\n",
    "##fig.update_layout(template='plotly_white', font=dict(family=\"Courier New, monospace\", size=12),\n",
    "##                  legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1, xanchor=\"right\", x=1))\n",
    "##pyo.plot(fig, filename=\"./results.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer:\n",
    "    \"\"\"A simple numpy replay buffer.\"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim: int, size: int, batch_size: int = 32):\n",
    "        self.obs_dim = obs_dim\n",
    "        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)\n",
    "        self.acts_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.rews_buf = np.zeros([size], dtype=np.float32)\n",
    "        self.done_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.max_size, self.batch_size = size, batch_size\n",
    "        self.ptr, self.size, = 0, 0\n",
    "\n",
    "    def store(\n",
    "        self,\n",
    "        obs: np.ndarray,\n",
    "        act: np.ndarray, \n",
    "        rew: float, \n",
    "        next_obs: np.ndarray, \n",
    "        done: bool,\n",
    "    ):\n",
    "        self.obs_buf[self.ptr] = obs.reshape(self.obs_buf[self.ptr].shape)\n",
    "        self.next_obs_buf[self.ptr] = next_obs.reshape(self.obs_buf[self.ptr].shape)\n",
    "        self.acts_buf[self.ptr] = act\n",
    "        self.rews_buf[self.ptr] = rew\n",
    "        self.done_buf[self.ptr] = done\n",
    "        self.ptr = (self.ptr + 1) % self.max_size\n",
    "        self.size = min(self.size + 1, self.max_size)\n",
    "\n",
    "    def sample_batch(self) -> Dict[str, np.ndarray]:\n",
    "        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)\n",
    "        return dict(obs=self.obs_buf[idxs],\n",
    "                    next_obs=self.next_obs_buf[idxs],\n",
    "                    acts=self.acts_buf[idxs],\n",
    "                    rews=self.rews_buf[idxs],\n",
    "                    done=self.done_buf[idxs])\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(nn.Module):\n",
    "    def __init__(self, in_dim: int, out_dim: int):\n",
    "        \"\"\"Initialization.\"\"\"\n",
    "        super(Network, self).__init__()\n",
    "\n",
    "        self.layers = nn.Sequential(\n",
    "            nn.Linear(in_dim, 128), \n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128), \n",
    "            nn.ReLU(), \n",
    "            nn.Linear(128, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Forward method implementation.\"\"\"\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent:\n",
    "    \"\"\"DQN Agent interacting with environment.\n",
    "    \n",
    "    Attribute:\n",
    "        env : (Environment) custom Environment to interact with TRNSYS\n",
    "        memory (ReplayBuffer): replay memory to store transitions\n",
    "        batch_size (int): batch size for sampling\n",
    "        epsilon (float): parameter for epsilon greedy policy\n",
    "        epsilon_decay (float): step size to decrease epsilon\n",
    "        max_epsilon (float): max value of epsilon\n",
    "        min_epsilon (float): min value of epsilon\n",
    "        target_update (int): period for target model's hard update\n",
    "        gamma (float): discount factor\n",
    "        dqn (Network): model to train and select actions\n",
    "        dqn_target (Network): target model to update\n",
    "        optimizer (torch.optim): optimizer for training dqn\n",
    "        transition (list): transition information including \n",
    "                           state, action, reward, next_state, done\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        env: gym.Env,\n",
    "        memory_size: int,\n",
    "        batch_size: int,\n",
    "        target_update: int,\n",
    "        epsilon_decay: float,\n",
    "        max_epsilon: float = 1.0,\n",
    "        min_epsilon: float = 0.1,\n",
    "        gamma: float = 0.99,\n",
    "    ):\n",
    "        \"\"\"Initialization.\n",
    "        \n",
    "        Args:\n",
    "            env (gym.Env): custom Environment to interact with TRNSYS\n",
    "            memory_size (int): length of memory\n",
    "            batch_size (int): batch size for sampling\n",
    "            target_update (int): period for target model's hard update\n",
    "            epsilon_decay (float): step size to decrease epsilon\n",
    "            lr (float): learning rate\n",
    "            max_epsilon (float): max value of epsilon\n",
    "            min_epsilon (float): min value of epsilon\n",
    "            gamma (float): discount factor\n",
    "        \"\"\"\n",
    "        ## dimensions for the network\n",
    "        obs_dim = env.observation_dim\n",
    "        action_dim = env.action_dim\n",
    "        \n",
    "        self.env = env\n",
    "        self.memory = ReplayBuffer(obs_dim, memory_size, batch_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.epsilon = max_epsilon\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.max_epsilon = max_epsilon\n",
    "        self.min_epsilon = min_epsilon\n",
    "        self.target_update = target_update\n",
    "        self.gamma = gamma\n",
    "        \n",
    "        # device: cpu / gpu\n",
    "        self.device = torch.device(\n",
    "            \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        )\n",
    "        print(self.device)\n",
    "\n",
    "        # networks: dqn, dqn_target\n",
    "        self.dqn = Network(obs_dim, action_dim).to(self.device)\n",
    "        self.dqn_target = Network(obs_dim, action_dim).to(self.device)\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "        self.dqn_target.eval()\n",
    "        \n",
    "        # optimizer\n",
    "        self.optimizer = optim.Adam(self.dqn.parameters())\n",
    "\n",
    "        # transition to store in memory\n",
    "        self.transition = list()\n",
    "        \n",
    "        # mode: train / test\n",
    "        self.is_test = False\n",
    "\n",
    "    def select_action(self, state: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"Select an action from the input state.\"\"\"\n",
    "        # epsilon greedy policy\n",
    "        if self.epsilon > np.random.random():\n",
    "            selected_action = np.random.choice(self.env.action_dim,1)[0]\n",
    "        else:\n",
    "            selected_action = self.dqn(\n",
    "                torch.FloatTensor(state.T).to(self.device)\n",
    "            ).argmax()\n",
    "            selected_action = selected_action.detach().cpu().numpy()\n",
    "        \n",
    "        if not self.is_test:\n",
    "            self.transition = [state, selected_action]\n",
    "        \n",
    "        return selected_action\n",
    "\n",
    "    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:\n",
    "        \"\"\"Take an action and return the response of the env.\"\"\"\n",
    "        next_state, reward, done, info = self.env.step(action)\n",
    "\n",
    "        if not self.is_test:\n",
    "            self.transition += [reward, next_state, done]\n",
    "            self.memory.store(*self.transition)\n",
    "    \n",
    "        return next_state, reward, done, info\n",
    "\n",
    "    def update_model(self) -> torch.Tensor:\n",
    "        \"\"\"Update the model by gradient descent.\"\"\"\n",
    "        samples = self.memory.sample_batch()\n",
    "\n",
    "        loss = self._compute_dqn_loss(samples)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "        \n",
    "    def train(self):\n",
    "        \"\"\"Train the agent.\"\"\"\n",
    "        self.is_test = False\n",
    "        \n",
    "        state = self.env.reset()\n",
    "        update_cnt = 0\n",
    "        epsilons = []\n",
    "        losses = []\n",
    "        tair = []\n",
    "        actions = []\n",
    "        pmv = []\n",
    "        qheat = []\n",
    "        rewards = []\n",
    "        occ = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        for _ in range(env.numsteps):\n",
    "            action = self.select_action(state)\n",
    "            next_state, reward, done,info = self.step(action)\n",
    "\n",
    "\n",
    "            rewards.append(reward)\n",
    "            actions.append(env.action_to_temp[action])\n",
    "            pmv.append(info['pmv'][0])\n",
    "            d = env.observation_to_dict(next_state)\n",
    "            tair.append(d[\"tair_in\"][0])\n",
    "            qheat.append(d[\"qheat_in\"][0])\n",
    "            occ.append(d[\"occ_in\"][0])\n",
    "            \n",
    "\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "            # if episode ends\n",
    "            if done:\n",
    "                state = self.env.reset()\n",
    "\n",
    "            # if training is ready\n",
    "            if len(self.memory) >= self.batch_size:\n",
    "                loss = self.update_model()\n",
    "                losses.append(loss)\n",
    "                update_cnt += 1\n",
    "                \n",
    "                # linearly decrease epsilon\n",
    "                self.epsilon = max(\n",
    "                    self.min_epsilon, self.epsilon - (\n",
    "                        self.max_epsilon - self.min_epsilon\n",
    "                    ) * self.epsilon_decay\n",
    "                )\n",
    "                epsilons.append(self.epsilon)\n",
    "                \n",
    "                # if hard update is needed\n",
    "                if update_cnt % self.target_update == 0:\n",
    "                    self._target_hard_update()\n",
    "\n",
    "\n",
    "        \n",
    "        ## getting current date for logging reasons\n",
    "        date = datetime.now()\n",
    "        temp = list([date.year,date.month,date.day,date.hour,date.minute])\n",
    "        temp = [str(x) for x in temp]\n",
    "        time = \"_\".join(temp)\n",
    "        RESULT_PATH = \"./results/results_{time}\"\n",
    "        os.makedirs(RESULT_PATH,exist_ok=True)\n",
    "\n",
    "\n",
    "\n",
    "        plot_filename=f\"{RESULT_PATH}/results_{time}.html\"\n",
    "        self._plot(epsilons,losses,tair,actions,pmv,qheat,rewards,occ,filename = plot_filename)\n",
    "\n",
    "\n",
    "        ## saving parameters of environment\n",
    "\n",
    "        f = open(f\"{RESULT_PATH}/env_params_{time}.json\",\"w\")\n",
    "        f.write(json.dumps(env.log_dict,indent=True))\n",
    "        f.close()\n",
    "\n",
    "\n",
    "        ## TODO SAVE TRAINING DATA\n",
    "                \n",
    "        self.env.close()\n",
    "\n",
    "\n",
    "    def _plot(\n",
    "        self, \n",
    "        epsilons: List[float],\n",
    "        losses : List[float],\n",
    "        tair: List[float],\n",
    "        actions: List[float],\n",
    "        pmv : List[float],\n",
    "        qheat: List[float],\n",
    "        rewards: List[float],\n",
    "        occ : List[float],\n",
    "        plot_filename:str\n",
    "       ):\n",
    "\n",
    "        epsilons = np.array(epsilons)\n",
    "        losses = np.array(losses)\n",
    "        tair= np.array(tair)\n",
    "        actions = np.array(actions)\n",
    "        pmv = np.array(pmv)\n",
    "        qheat = np.array(qheat)\n",
    "        rewards = np.array(rewards)\n",
    "        occ = np.array(occ)\n",
    "\n",
    "\n",
    "        # Plotting the summary of simulation\n",
    "        fig = make_subplots(rows=8, cols=1, shared_xaxes=True, vertical_spacing=0.04,\n",
    "                            specs=[[{\"secondary_y\": False}], [{\"secondary_y\": False}], [{\"secondary_y\": False}],\n",
    "                                   [{\"secondary_y\": True}], [{\"secondary_y\": True}], [{\"secondary_y\": False}],\n",
    "                                   [{\"secondary_y\": True}], [{\"secondary_y\": True}]])\n",
    "        \n",
    "        iterations = len(tair)\n",
    "        t = np.linspace(0.0, iterations -1, iterations)\n",
    "        # Add traces\n",
    "        fig.add_trace(go.Scatter(name='Tair(state)', x=t, y=tair.flatten(), mode='lines', line=dict(width=1, color='cyan')),\n",
    "                      row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(name='Tair_avg', x=t, y=pd.Series(tair.flatten()).rolling(window=24).mean(), mode='lines',\n",
    "                      line=dict(width=2, color='blue')), row=1, col=1)\n",
    "        fig.add_trace(go.Scatter(name='Tset(action)', x=t, y=actions.flatten(), mode='lines', line=dict(width=1, color='fuchsia')),\n",
    "                      row=2, col=1)\n",
    "        fig.add_trace(go.Scatter(name='Tset_avg', x=t, y=pd.Series(actions.flatten()).rolling(window=24).mean(), mode='lines',\n",
    "                      line=dict(width=2, color='purple')), row=2, col=1)\n",
    "        fig.add_trace(go.Scatter(name='Pmv', x=t, y=pmv.flatten(), mode='lines', line=dict(width=1, color='gold')),\n",
    "                      row=3, col=1)\n",
    "        fig.add_trace(go.Scatter(name='Pmv_avg', x=t, y=pd.Series(pmv.flatten()).rolling(window=24).mean(), mode='lines',\n",
    "                      line=dict(width=2, color='darkorange')), row=3, col=1)\n",
    "        fig.add_trace(go.Scatter(name='Heating', x=t, y=qheat.flatten(), mode='lines', line=dict(width=1, color='red')),\n",
    "                      row=4, col=1, secondary_y=False)\n",
    "        fig.add_trace(go.Scatter(name='Heating_cumulative', x=t, y=np.cumsum(qheat.flatten()), mode='lines',\n",
    "                      line=dict(width=2, color='darkred')), row=4, col=1, secondary_y=True)\n",
    "        fig.add_trace(go.Scatter(name='Reward', x=t, y=rewards.flatten(), mode='lines', line=dict(width=1, color='lime')),\n",
    "                      row=5, col=1, secondary_y=False)\n",
    "        fig.add_trace(go.Scatter(name='Reward_cum', x=t, y=np.cumsum(rewards.flatten()), mode='lines',\n",
    "                      line=dict(width=2, color='darkgreen')), row=5, col=1, secondary_y=True)\n",
    "        fig.add_trace(go.Scatter(name='Occupancy', x=t, y=occ.flatten(), mode='lines',\n",
    "                      line=dict(width=1, color='black')), row=6, col=1)\n",
    "        ## training part\n",
    "\n",
    "        fig.add_trace(go.Scatter(name='Epsilons', x=t, y=epsilons.flatten(), mode='lines',\n",
    "                      line=dict(width=1, color='blue')), row=7, col=1)\n",
    "        fig.add_trace(go.Scatter(name='Training Loss', x=t, y=losses.flatten(), mode='lines',\n",
    "                      line=dict(width=1, color='darkblue')), row=8, col=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        # Set x-axis title\n",
    "        fig.update_xaxes(title_text=\"Timestep (-)\", row=6, col=1)\n",
    "        # Set y-axes titles\n",
    "        fig.update_yaxes(title_text=\"<b>Tair</b> (째C)\", range=[10, 24], row=1, col=1)\n",
    "        fig.update_yaxes(title_text=\"<b>Tset</b> (째C)\", range=[12, 30], row=2, col=1)\n",
    "        fig.update_yaxes(title_text=\"<b>PMV</b> (-)\", row=3, col=1)\n",
    "        fig.update_yaxes(title_text=\"<b>Heat Power</b> (kJ/hr)\", row=4, col=1, secondary_y=False)\n",
    "        fig.update_yaxes(title_text=\"<b>Heat Energy</b> (kJ)\", row=4, col=1, secondary_y=True)\n",
    "        fig.update_yaxes(title_text=\"<b>Reward</b> (-)\", row=5, col=1, range=[-5, 5], secondary_y=False)\n",
    "        fig.update_yaxes(title_text=\"<b>Tot Reward</b> (-)\", row=5, col=1, secondary_y=True)\n",
    "        fig.update_yaxes(title_text=\"<b>Fraction</b> (-)\", row=6, col=1)\n",
    "        fig.update_yaxes(title_text=\"<b>Epsilon</b> (-)\", row=7, col=1)\n",
    "        fig.update_yaxes(title_text=\"<b>Loss</b> (-)\", row=8, col=1)\n",
    "\n",
    "\n",
    "        fig.update_xaxes(nticks=50)\n",
    "        fig.update_layout(template='plotly_white', font=dict(family=\"Courier New, monospace\", size=12),\n",
    "                          legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1, xanchor=\"right\", x=1))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "        ## saving plots\n",
    "        pyo.plot(fig, filename=plot_filename)\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "                \n",
    "\n",
    "    def _compute_dqn_loss(self, samples: Dict[str, np.ndarray]) -> torch.Tensor:\n",
    "        \"\"\"Return dqn loss.\"\"\"\n",
    "        device = self.device  # for shortening the following lines\n",
    "        state = torch.FloatTensor(samples[\"obs\"]).to(device)\n",
    "        next_state = torch.FloatTensor(samples[\"next_obs\"]).to(device)\n",
    "        action = torch.LongTensor(samples[\"acts\"].reshape(-1, 1)).to(device)\n",
    "        reward = torch.FloatTensor(samples[\"rews\"].reshape(-1, 1)).to(device)\n",
    "        done = torch.FloatTensor(samples[\"done\"].reshape(-1, 1)).to(device)\n",
    "\n",
    "        # G_t   = r + gamma * v(s_{t+1})  if state != Terminal\n",
    "        #       = r                       otherwise\n",
    "        curr_q_value = self.dqn(state).gather(1, action)\n",
    "        next_q_value = self.dqn_target(\n",
    "            next_state\n",
    "        ).max(dim=1, keepdim=True)[0].detach()\n",
    "        mask = 1 - done\n",
    "        target = (reward + self.gamma * next_q_value * mask).to(self.device)\n",
    "\n",
    "        # calculate dqn loss\n",
    "        loss = F.smooth_l1_loss(curr_q_value, target)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def _target_hard_update(self):\n",
    "        \"\"\"Hard update: target <- local.\"\"\"\n",
    "        self.dqn_target.load_state_dict(self.dqn.state_dict())\n",
    "\n",
    "\n",
    "    # Making a save method to save a trained model\n",
    "    def save(self, filename, directory):\n",
    "        torch.save(self.dqn.state_dict(), '%s/%s_dqn.pth' % (directory, filename))\n",
    "        torch.save(self.dqn_target.state_dict(), '%s/%s_dqn_target.pth' % (directory, filename))\n",
    "\n",
    "    # Making a load method to load a pre-trained model\n",
    "    def load(self, filename, directory):\n",
    "        self.dqn.load_state_dict(torch.load('%s/%s_dqn.pth' % (directory, filename)))\n",
    "        self.dqn_target.load_state_dict(torch.load('%s/%s_dqn_target.pth' % (directory, filename)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[777]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "seed = 777\n",
    "env = gym.make('EnergyPlusEnv-v0')\n",
    "\n",
    "def seed_torch(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.backends.cudnn.enabled:\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "\n",
    "np.random.seed(seed)\n",
    "seed_torch(seed)\n",
    "env.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_keep = ['modelname','numsteps','timestop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "memory_size = 1000\n",
    "batch_size = 32\n",
    "target_update = 100\n",
    "epsilon_decay = 1 / 2000\n",
    "\n",
    "agent = DQNAgent(env,memory_size,batch_size,target_update,epsilon_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of envs.energyplus_env_dir.energyplus_env failed: Traceback (most recent call last):\n",
      "  File \"C:\\Users\\Harold\\anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 245, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"C:\\Users\\Harold\\anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 410, in superreload\n",
      "    update_generic(old_obj, new_obj)\n",
      "  File \"C:\\Users\\Harold\\anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"C:\\Users\\Harold\\anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 302, in update_class\n",
      "    if update_generic(old_obj, new_obj): continue\n",
      "  File \"C:\\Users\\Harold\\anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 347, in update_generic\n",
      "    update(a, b)\n",
      "  File \"C:\\Users\\Harold\\anaconda3\\lib\\site-packages\\IPython\\extensions\\autoreload.py\", line 266, in update_function\n",
      "    setattr(old, name, getattr(new, name))\n",
      "ValueError: __init__() requires a code object with 1 free vars, not 0\n",
      "]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'env': <envs.energyplus_env_dir.energyplus_env.EnergyPlusEnv at 0x18a83156610>,\n",
       " '_action_space': None,\n",
       " '_observation_space': None,\n",
       " '_reward_range': None,\n",
       " '_metadata': None,\n",
       " '_has_reset': True}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e78a7ef29a5e3028f948eff69c34ba1d8ebd35a887497a02775c6aab840f6bc2"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
