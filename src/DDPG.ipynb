{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Harold\\AppData\\Local\\Temp\\ipykernel_212\\3777615979.py:1: DeprecationWarning: Importing display from IPython.core.display is deprecated since IPython 7.14, please import from IPython display\n",
      "  from IPython.core.display import display, HTML\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPG Thermal Control Policy Implementation\n",
    "\n",
    "# Importing the libraries\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from pyfmi import load_fmu\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Initializing the Experience Replay Memory\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, max_size=1e6):\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "\n",
    "    def add(self, transition):\n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = transition\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        batch_states, batch_next_states, batch_actions, batch_rewards = [], [], [], []\n",
    "        for i in ind:\n",
    "            state, next_state, action, reward = self.storage[i]\n",
    "            batch_states.append(np.array(state, copy=False))\n",
    "            batch_next_states.append(np.array(next_state, copy=False))\n",
    "            batch_actions.append(np.array(action, copy=False))\n",
    "            batch_rewards.append(np.array(reward, copy=False))\n",
    "        return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(\n",
    "            batch_rewards).reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Building a neural network for the actor model and a neural network for the actor target\n",
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer_1 = nn.Linear(state_dim, 400)\n",
    "        self.layer_2 = nn.Linear(400, 300)\n",
    "        self.layer_3 = nn.Linear(300, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer_1(x))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        x = self.max_action * torch.tanh(self.layer_3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Building a neural network for the critic model and a neural network for the critic target\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.layer_2 = nn.Linear(400, 300)\n",
    "        self.layer_3 = nn.Linear(300, 1)\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "        #print(xu.size())\n",
    "        #print(x.size(), u.size())\n",
    "        x1 = F.relu(self.layer_1(xu))\n",
    "        x1 = F.relu(self.layer_2(x1))\n",
    "        x1 = self.layer_3(x1)\n",
    "        return x1\n",
    "\n",
    "\n",
    "# Selecting the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Building the whole DDPG Training Process into a class\n",
    "class DDPG(object):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "        # Learning Process for the DDPG Algorithm\n",
    "\n",
    "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2,\n",
    "              noise_clip=0.5):\n",
    "\n",
    "        for it in range(iterations):\n",
    "\n",
    "            # Step 4: We sample a batch of transitions (s, s', a, r) from the memory\n",
    "            batch_states, batch_next_states, batch_actions, batch_rewards = replay_buffer.sample(\n",
    "                batch_size)\n",
    "            state = torch.Tensor(batch_states).to(device)\n",
    "            next_state = torch.Tensor(batch_next_states).to(device)\n",
    "            action = torch.Tensor(batch_actions).to(device)\n",
    "            reward = torch.Tensor(batch_rewards).to(device)\n",
    "\n",
    "            # Step 5: From the next state s', the Actor target plays the next action a'\n",
    "            next_action = self.actor_target(next_state)\n",
    "            #print(next_action.size())\n",
    "\n",
    "            # Step 6: We add Gaussian noise to this next action a' and we clamp it in a range of values supported\n",
    "            # by the environment\n",
    "            noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
    "            noise = noise.clamp(-noise_clip, noise_clip)\n",
    "            next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            # Step 7: The Critic Target take (s', a') as input and return Q-value Qt(s', a') as output\n",
    "            target_q = self.critic_target(next_state, next_action)\n",
    "\n",
    "            # Step 8: We get the estimated reward, which is: r' = r + γ * Qt, where γ id the discount factor\n",
    "            target_q = reward + (discount * target_q).detach()\n",
    "\n",
    "            # Step 9: The Critic models take (s, a) as input and return Q-value Q(s, a) as output\n",
    "            current_q = self.critic(state, action)\n",
    "\n",
    "            # Step 10: We compute the loss coming from the Critic model: Critic Loss = MSE_Loss(Q(s,a), Qt)\n",
    "            critic_loss = F.mse_loss(current_q, target_q)\n",
    "\n",
    "            # Step 11: We back propagate this Critic loss and update the parameters of the Critic model with a SGD\n",
    "            # optimizer\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            # Step 12: We update our Actor model by performing gradient ascent on the output of the first Critic model\n",
    "            actor_loss = -self.critic.forward(state, self.actor(state)).mean()\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # Step 13: We update the weights of the Actor target by polyak averaging\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "            # Step 14: We update the weights of the Critic target by polyak averaging\n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "    # Making a save method to save a trained model\n",
    "    def save(self, filename, directory):\n",
    "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "\n",
    "    # Making a load method to load a pre-trained model\n",
    "    def load(self, filename, directory):\n",
    "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "Settings: DDPG_DIET\n",
      "---------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# We set the parameters\n",
    "save_models = True  # Boolean checker whether or not to save the pre-trained model\n",
    "expl_noise = 0.1  # Exploration noise - STD value of exploration Gaussian noise\n",
    "batch_size = 100  # Size of the batch\n",
    "discount = 0.99  # Discount factor gamma, used in the calculation of the total discounted reward\n",
    "tau = 0.005  # Target network update rate\n",
    "policy_noise = 0.2  # STD of Gaussian noise added to the actions for the exploration purposes\n",
    "noise_clip = 0.5  # Maximum value of the Gaussian noise added to the actions (policy)\n",
    "alpha = 0.5  # Adjusting co-efficient for the comfort reward\n",
    "beta = 1  # Adjusting co-efficient for the energy reward\n",
    "obs = np.array([20.0, 50.0, 20.0, 0.1, 5.5, 1.0, 1.0, 0.0001, 0.0001])  # Initial state of the trnsys env\n",
    "modelname = 'CELLS_v1'\n",
    "days = 90  # Number of days the simulation is run for\n",
    "hours = 24  # Number of hours each day the simulation is run for\n",
    "minutes = 60\n",
    "seconds = 60\n",
    "ep_timestep = 4  # Number of timesteps per hour\n",
    "numsteps = days * hours * ep_timestep\n",
    "timestop = days * hours * minutes * seconds\n",
    "secondstep = timestop / numsteps\n",
    "\n",
    "# We create a filename for the two saved models: the Actor and Critic Models\n",
    "file_name = \"%s_%s\" % (\"DDPG\", \"DIET\")\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Settings: %s\" % file_name)\n",
    "print(\"---------------------------------------\")\n",
    "\n",
    "# We get the necessary information on the states and actions in the chosen environment\n",
    "state_dim = 6\n",
    "action_dim = 1\n",
    "max_action = 21\n",
    "\n",
    "# We create the policy network (the Actor model)\n",
    "policy = DDPG(state_dim, action_dim, max_action)\n",
    "\n",
    "# We create the Experience Replay memory\n",
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function taken from CBE comfort tool to calculate the pmv value for comfort evaluation\n",
    "def comfPMV(ta, tr, rh, vel=0.1, met=1.1, clo=1, wme=0):\n",
    "    pa = rh * 10 * math.exp(16.6536 - 4030.183 / (ta + 235))\n",
    "\n",
    "    icl = 0.155 * clo  # thermal insulation of the clothing in M2K/W\n",
    "    m = met * 58.15  # metabolic rate in W/M2\n",
    "    w = wme * 58.15  # external work in W/M2\n",
    "    mw = m - w  # internal heat production in the human body\n",
    "    if icl <= 0.078:\n",
    "        fcl = 1 + (1.29 * icl)\n",
    "    else:\n",
    "        fcl = 1.05 + (0.645 * icl)\n",
    "\n",
    "    # heat transfer coefficient by forced convection\n",
    "    hcf = 12.1 * math.sqrt(vel)\n",
    "    taa = ta + 273\n",
    "    tra = tr + 273\n",
    "    # we have verified that using the equation below or this tcla = taa + (35.5 - ta) / (3.5 * (6.45 * icl + .1))\n",
    "    # does not affect the PMV value\n",
    "    tcla = taa + (35.5 - ta) / (3.5 * icl + 0.1)\n",
    "\n",
    "    p1 = icl * fcl\n",
    "    p2 = p1 * 3.96\n",
    "    p3 = p1 * 100\n",
    "    p4 = p1 * taa\n",
    "    p5 = (308.7 - 0.028 * mw) + (p2 * math.pow(tra / 100.0, 4))\n",
    "    xn = tcla / 100\n",
    "    xf = tcla / 50\n",
    "    eps = 0.00015\n",
    "\n",
    "    n = 0\n",
    "    while abs(xn - xf) > eps:\n",
    "        xf = (xf + xn) / 2\n",
    "        hcn = 2.38 * math.pow(abs(100.0 * xf - taa), 0.25)\n",
    "        if hcf > hcn:\n",
    "            hc = hcf\n",
    "        else:\n",
    "            hc = hcn\n",
    "        xn = (p5 + p4 * hc - p2 * math.pow(xf, 4)) / (100 + p3 * hc)\n",
    "        n += 1\n",
    "        if n > 150:\n",
    "            print('Max iterations exceeded')\n",
    "            return 1  # fixme should not return 1 but instead PMV=999 as per ashrae standard\n",
    "\n",
    "    tcl = 100 * xn - 273\n",
    "\n",
    "    # heat loss diff. through skin\n",
    "    hl1 = 3.05 * 0.001 * (5733 - (6.99 * mw) - pa)\n",
    "    # heat loss by sweating\n",
    "    if mw > 58.15:\n",
    "        hl2 = 0.42 * (mw - 58.15)\n",
    "    else:\n",
    "        hl2 = 0\n",
    "    # latent respiration heat loss\n",
    "    hl3 = 1.7 * 0.00001 * m * (5867 - pa)\n",
    "    # dry respiration heat loss\n",
    "    hl4 = 0.0014 * m * (34 - ta)\n",
    "    # heat loss by radiation\n",
    "    hl5 = 3.96 * fcl * (math.pow(xn, 4) - math.pow(tra / 100.0, 4))\n",
    "    # heat loss by convection\n",
    "    hl6 = fcl * hc * (tcl - ta)\n",
    "\n",
    "    ts = 0.303 * math.exp(-0.036 * m) + 0.028\n",
    "    pmv = ts * (mw - hl1 - hl2 - hl3 - hl4 - hl5 - hl6)\n",
    "    ppd = 100.0 - 95.0 * math.exp(-0.03353 * pow(pmv, 4.0) - 0.2179 * pow(pmv, 2.0))\n",
    "\n",
    "    return pmv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total timesteps: 8640 Episode Num: 1 Reward: 0.0\n",
      "Total timesteps: 8640 Episode Num: 2 Reward: 0.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Harold\\Desktop\\ENAC-Semester-Project\\DIET_Controller\\src\\DDPG.ipynb Cell 5'\u001b[0m in \u001b[0;36m<cell line: 12>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Harold/Desktop/ENAC-Semester-Project/DIET_Controller/src/DDPG.ipynb#ch0000004?line=40'>41</a>\u001b[0m \u001b[39mif\u001b[39;00m sim_num \u001b[39m!=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Harold/Desktop/ENAC-Semester-Project/DIET_Controller/src/DDPG.ipynb#ch0000004?line=41'>42</a>\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mTotal timesteps: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m Episode Num: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m Reward: \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(numsteps, sim_num, np\u001b[39m.\u001b[39msum(reward\u001b[39m.\u001b[39mflatten())))\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Harold/Desktop/ENAC-Semester-Project/DIET_Controller/src/DDPG.ipynb#ch0000004?line=42'>43</a>\u001b[0m     policy\u001b[39m.\u001b[39;49mtrain(replay_buffer, (numsteps \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m) \u001b[39m*\u001b[39;49m sim_num, batch_size, discount, tau, policy_noise, noise_clip)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Harold/Desktop/ENAC-Semester-Project/DIET_Controller/src/DDPG.ipynb#ch0000004?line=44'>45</a>\u001b[0m     \u001b[39m#if sim_num > 1:\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Harold/Desktop/ENAC-Semester-Project/DIET_Controller/src/DDPG.ipynb#ch0000004?line=45'>46</a>\u001b[0m         \u001b[39m#policy.save(file_name, directory=\"./pytorch_models/01032022\")  # Change the folder name here\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Harold/Desktop/ENAC-Semester-Project/DIET_Controller/src/DDPG.ipynb#ch0000004?line=47'>48</a>\u001b[0m \u001b[39mwhile\u001b[39;00m simtime \u001b[39m<\u001b[39m timestop:\n",
      "\u001b[1;32mc:\\Users\\Harold\\Desktop\\ENAC-Semester-Project\\DIET_Controller\\src\\DDPG.ipynb Cell 2'\u001b[0m in \u001b[0;36mDDPG.train\u001b[1;34m(self, replay_buffer, iterations, batch_size, discount, tau, policy_noise, noise_clip)\u001b[0m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Harold/Desktop/ENAC-Semester-Project/DIET_Controller/src/DDPG.ipynb#ch0000001?line=140'>141</a>\u001b[0m \u001b[39m# Step 11: We back propagate this Critic loss and update the parameters of the Critic model with a SGD\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Harold/Desktop/ENAC-Semester-Project/DIET_Controller/src/DDPG.ipynb#ch0000001?line=141'>142</a>\u001b[0m \u001b[39m# optimizer\u001b[39;00m\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Harold/Desktop/ENAC-Semester-Project/DIET_Controller/src/DDPG.ipynb#ch0000001?line=142'>143</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic_optimizer\u001b[39m.\u001b[39mzero_grad()\n\u001b[1;32m--> <a href='vscode-notebook-cell:/c%3A/Users/Harold/Desktop/ENAC-Semester-Project/DIET_Controller/src/DDPG.ipynb#ch0000001?line=143'>144</a>\u001b[0m critic_loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Harold/Desktop/ENAC-Semester-Project/DIET_Controller/src/DDPG.ipynb#ch0000001?line=144'>145</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcritic_optimizer\u001b[39m.\u001b[39mstep()\n\u001b[0;32m    <a href='vscode-notebook-cell:/c%3A/Users/Harold/Desktop/ENAC-Semester-Project/DIET_Controller/src/DDPG.ipynb#ch0000001?line=146'>147</a>\u001b[0m \u001b[39m# Step 12: We update our Actor model by performing gradient ascent on the output of the first Critic model\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Harold\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:307\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Harold/anaconda3/lib/site-packages/torch/_tensor.py?line=297'>298</a>\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[0;32m    <a href='file:///c%3A/Users/Harold/anaconda3/lib/site-packages/torch/_tensor.py?line=298'>299</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    <a href='file:///c%3A/Users/Harold/anaconda3/lib/site-packages/torch/_tensor.py?line=299'>300</a>\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[0;32m    <a href='file:///c%3A/Users/Harold/anaconda3/lib/site-packages/torch/_tensor.py?line=300'>301</a>\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Harold/anaconda3/lib/site-packages/torch/_tensor.py?line=304'>305</a>\u001b[0m         create_graph\u001b[39m=\u001b[39mcreate_graph,\n\u001b[0;32m    <a href='file:///c%3A/Users/Harold/anaconda3/lib/site-packages/torch/_tensor.py?line=305'>306</a>\u001b[0m         inputs\u001b[39m=\u001b[39minputs)\n\u001b[1;32m--> <a href='file:///c%3A/Users/Harold/anaconda3/lib/site-packages/torch/_tensor.py?line=306'>307</a>\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs)\n",
      "File \u001b[1;32mc:\\Users\\Harold\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:154\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    <a href='file:///c%3A/Users/Harold/anaconda3/lib/site-packages/torch/autograd/__init__.py?line=150'>151</a>\u001b[0m \u001b[39mif\u001b[39;00m retain_graph \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    <a href='file:///c%3A/Users/Harold/anaconda3/lib/site-packages/torch/autograd/__init__.py?line=151'>152</a>\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m--> <a href='file:///c%3A/Users/Harold/anaconda3/lib/site-packages/torch/autograd/__init__.py?line=153'>154</a>\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(\n\u001b[0;32m    <a href='file:///c%3A/Users/Harold/anaconda3/lib/site-packages/torch/autograd/__init__.py?line=154'>155</a>\u001b[0m     tensors, grad_tensors_, retain_graph, create_graph, inputs,\n\u001b[0;32m    <a href='file:///c%3A/Users/Harold/anaconda3/lib/site-packages/torch/autograd/__init__.py?line=155'>156</a>\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#os.chdir(r'C:\\Users\\achatter\\Desktop\\PhDResearch\\DIET\\Controller\\Cosimulation_energyplus')\n",
    "os.chdir(r'C:\\Users\\Harold\\Desktop\\ENAC-Semester-Project\\DIET_Controller\\EnergyPlus_simulations\\simple_simulation')\n",
    "\n",
    "##model = load_fmu(modelname+'.fmu')\n",
    "##opts = model.simulate_options()  # Get the default options\n",
    "##opts['ncp'] = numsteps  # Specifies the number of timesteps\n",
    "##opts['initialize'] = False\n",
    "##simtime = 0\n",
    "##model.initialize(simtime, timestop)\n",
    "index = 0\n",
    "\n",
    "for sim_num in range(5):\n",
    "\n",
    "\n",
    "    model = load_fmu(modelname+'.fmu')\n",
    "    opts = model.simulate_options()  # Get the default options\n",
    "    opts['ncp'] = numsteps  # Specifies the number of timesteps\n",
    "    opts['initialize'] = False\n",
    "    simtime = 0\n",
    "    model.reset()\n",
    "    model.instantiate_slave()\n",
    "    model.initialize(simtime, timestop)\n",
    "    simtime = 0\n",
    "    index = 0\n",
    "\n",
    "\n",
    "    t = np.linspace(0.0, timestop, numsteps)/ minutes * seconds\n",
    "    inputcheck_heating = np.zeros((numsteps, 1))\n",
    "    tair = np.zeros((numsteps, 1))\n",
    "    rh = np.zeros((numsteps, 1))\n",
    "    tmrt = np.zeros((numsteps, 1))\n",
    "    tout = np.zeros((numsteps, 1))\n",
    "    occ = np.zeros((numsteps, 1))\n",
    "    qheat = np.zeros((numsteps, 1))\n",
    "    pmv = np.zeros((numsteps, 1))\n",
    "    reward = np.zeros((numsteps, 1))\n",
    "    action = np.zeros((numsteps, 1))\n",
    "    state = np.zeros((numsteps, 6))\n",
    "\n",
    "\n",
    "    if sim_num != 0:\n",
    "        print(\"Total timesteps: {} Episode Num: {} Reward: {}\".format(numsteps, sim_num, np.sum(reward.flatten())))\n",
    "        policy.train(replay_buffer, (numsteps - 1) * sim_num, batch_size, discount, tau, policy_noise, noise_clip)\n",
    "\n",
    "        #if sim_num > 1:\n",
    "            #policy.save(file_name, directory=\"./pytorch_models/01032022\")  # Change the folder name here\n",
    "\n",
    "    while simtime < timestop:\n",
    "\n",
    "        if sim_num < 1:\n",
    "            action[index] = round(random.uniform(16, 21), 1)  # Choosing random values between 12 and 24 deg\n",
    "\n",
    "        else:  # After 1 episode, we switch to the model\n",
    "            action_arr = policy.select_action(obs)\n",
    "            # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
    "            if expl_noise != 0:\n",
    "                action_arr = (action_arr + np.random.normal(0, expl_noise, size=1)).clip(12.0, 21.0)\n",
    "                action[index] = action_arr[0]\n",
    "\n",
    "        model.set('Thsetpoint_diet', action[index])\n",
    "        res = model.do_step(current_t=simtime, step_size=secondstep, new_step=True)        \n",
    "        tair[index], rh[index], tmrt[index], tout[index], qheat[index], occ[index], inputcheck_heating[index] = model.get(['Tair', 'RH', 'Tmrt', 'Tout', 'Qheat', 'Occ', 'Thsetpoint_diet'])\n",
    "        state[index][0], state[index][1], state[index][2], state[index][3], state[index][4], state[index][5] = tair[index], rh[index], tmrt[index], tout[index], occ[index], qheat[index]\n",
    "        pmv[index] = comfPMV(tair[index], tmrt[index], rh[index])\n",
    "        reward[index] = beta * (1 - (qheat[index] / 650)) + alpha * (1 - abs(pmv[index] + 0.5)) * int(bool(occ[index]))\n",
    "\n",
    "        if index == 0:\n",
    "            obs = np.array([tair[index], rh[index], tmrt[index], tout[index], occ[index], qheat[index]]).flatten()\n",
    "\n",
    "        else:\n",
    "            new_obs = np.array([tair[index], rh[index], tmrt[index], tout[index], occ[index], qheat[index]]).flatten()\n",
    "            # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
    "            replay_buffer.add((obs, new_obs, action[index], reward[index]))\n",
    "            obs = new_obs\n",
    "\n",
    "        simtime += secondstep\n",
    "        index += 1\n",
    "\n",
    "    # Writing to .csv files to save the data from the episode\n",
    "    ##np.savetxt(\"./Training_Data/01032022/Ep\"+str(sim_num+1)+\"/state.csv\", state[:-1, :], delimiter=\",\")\n",
    "    ##np.savetxt(\"./Training_Data/01032022/Ep\"+str(sim_num+1)+\"/next_state.csv\", state[1:, :], delimiter=\",\")\n",
    "    ##np.savetxt(\"./Training_Data/01032022/Ep\"+str(sim_num+1)+\"/action.csv\", action[1:, :], delimiter=\",\")\n",
    "    ##np.savetxt(\"./Training_Data/01032022/Ep\"+str(sim_num+1)+\"/reward.csv\", reward[1:, :], delimiter=\",\")\n",
    "    ##np.savetxt(\"./Training_Data/01032022/Ep\"+str(sim_num+1)+\"/pmv.csv\", pmv[1:, :], delimiter=\",\")\n",
    "\n",
    "    # Plotting the summary of simulation\n",
    "    fig = make_subplots(rows=6, cols=1, shared_xaxes=True, vertical_spacing=0.04, specs=[[{\"secondary_y\": False}], [{\"secondary_y\": False}], [{\"secondary_y\": False}],\n",
    "                                                                                         [{\"secondary_y\": True}], [{\"secondary_y\": True}], [{\"secondary_y\": False}]])\n",
    "\n",
    "    # Add traces\n",
    "    fig.add_trace(go.Scatter(name='Tair(state)', x=t, y=tair.flatten(), mode='lines', line=dict(width=1, color='cyan')), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(name='Tair_avg', x=t, y=pd.Series(tair.flatten()).rolling(window=24).mean(), mode='lines', line=dict(width=2, color='blue')), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(name='Tset(action)', x=t, y=action.flatten(), mode='lines', line=dict(width=1, color='fuchsia')), row=2, col=1)\n",
    "    fig.add_trace(go.Scatter(name='Tset_avg', x=t, y=pd.Series(action.flatten()).rolling(window=24).mean(), mode='lines', line=dict(width=2, color='purple')), row=2, col=1)\n",
    "    fig.add_trace(go.Scatter(name='Pmv', x=t, y=pmv.flatten(), mode='lines', line=dict(width=1, color='gold')), row=3, col=1)\n",
    "    fig.add_trace(go.Scatter(name='Pmv_avg', x=t, y=pd.Series(pmv.flatten()).rolling(window=24).mean(), mode='lines', line=dict(width=2, color='darkorange')), row=3, col=1)\n",
    "    fig.add_trace(go.Scatter(name='Heating', x=t, y=qheat.flatten(), mode='lines', line=dict(width=1, color='red')), row=4, col=1, secondary_y=False)\n",
    "    fig.add_trace(go.Scatter(name='Heating_cumulative', x=t, y=np.cumsum(qheat.flatten()), mode='lines', line=dict(width=2, color='darkred')), row=4, col=1, secondary_y=True)\n",
    "    fig.add_trace(go.Scatter(name='Reward', x=t, y=reward.flatten(), mode='lines', line=dict(width=1, color='lime')), row=5, col=1, secondary_y=False)\n",
    "    fig.add_trace(go.Scatter(name='Reward_cum', x=t, y=np.cumsum(reward.flatten()), mode='lines', line=dict(width=2, color='darkgreen')), row=5, col=1, secondary_y=True)\n",
    "    fig.add_trace(go.Scatter(name='Occupancy', x=t, y=occ.flatten(), mode='lines', line=dict(width=1, color='black')), row=6, col=1)\n",
    "\n",
    "    # Set x-axis title\n",
    "    fig.update_xaxes(title_text=\"Timestep (-)\", row=6, col=1)\n",
    "    fig.update_xaxes(nticks=50)\n",
    "\n",
    "    # Set y-axes titles\n",
    "    fig.update_yaxes(title_text=\"<b>Tair</b> (°C)\", range=[10, 24], row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"<b>Tset</b> (°C)\", range=[12, 21], row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"<b>PMV</b> (-)\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"<b>Heat Power</b> (kJ/hr)\", row=4, col=1, secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"<b>Heat Energy</b> (kJ)\", row=4, col=1, secondary_y=True)\n",
    "    fig.update_yaxes(title_text=\"<b>Reward</b> (-)\", row=5, col=1, range=[-3, 3], secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"<b>Tot Reward</b> (-)\", row=5, col=1, secondary_y=True)\n",
    "    fig.update_yaxes(title_text=\"<b>Fraction</b> (-)\", row=6, col=1)    \n",
    "\n",
    "    fig.update_layout(template='plotly_white', font=dict(family=\"Courier New, monospace\", size=12), legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1, xanchor=\"right\", x=1))\n",
    "    pyo.plot(fig, filename=\"./results.html\")\n",
    "\n",
    "#policy.save(file_name, directory=\"./pytorch_models/01032022\")  # Change the folder name here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(qheat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6b6b609373cab7b4f35f89e556fae7285533d0878866e99b2da014c0f4ef9f7d"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
