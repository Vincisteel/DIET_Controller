{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DDPG Thermal Control Policy Implementation\n",
    "\n",
    "# Importing the libraries\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "from pyfmi import load_fmu\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.offline as pyo\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Initializing the Experience Replay Memory\n",
    "class ReplayBuffer(object):\n",
    "\n",
    "    def __init__(self, max_size=1e6):\n",
    "        self.storage = []\n",
    "        self.max_size = max_size\n",
    "        self.ptr = 0\n",
    "\n",
    "    def add(self, transition):\n",
    "        if len(self.storage) == self.max_size:\n",
    "            self.storage[int(self.ptr)] = transition\n",
    "            self.ptr = (self.ptr + 1) % self.max_size\n",
    "        else:\n",
    "            self.storage.append(transition)\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        ind = np.random.randint(0, len(self.storage), size=batch_size)\n",
    "        batch_states, batch_next_states, batch_actions, batch_rewards = [], [], [], []\n",
    "        for i in ind:\n",
    "            state, next_state, action, reward = self.storage[i]\n",
    "            batch_states.append(np.array(state, copy=False))\n",
    "            batch_next_states.append(np.array(next_state, copy=False))\n",
    "            batch_actions.append(np.array(action, copy=False))\n",
    "            batch_rewards.append(np.array(reward, copy=False))\n",
    "        return np.array(batch_states), np.array(batch_next_states), np.array(batch_actions), np.array(\n",
    "            batch_rewards).reshape(-1, 1)\n",
    "\n",
    "\n",
    "# Building a neural network for the actor model and a neural network for the actor target\n",
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        super(Actor, self).__init__()\n",
    "        self.layer_1 = nn.Linear(state_dim, 400)\n",
    "        self.layer_2 = nn.Linear(400, 300)\n",
    "        self.layer_3 = nn.Linear(300, action_dim)\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.layer_1(x))\n",
    "        x = F.relu(self.layer_2(x))\n",
    "        x = self.max_action * torch.tanh(self.layer_3(x))\n",
    "        return x\n",
    "\n",
    "\n",
    "# Building a neural network for the critic model and a neural network for the critic target\n",
    "class Critic(nn.Module):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super(Critic, self).__init__()\n",
    "        self.layer_1 = nn.Linear(state_dim + action_dim, 400)\n",
    "        self.layer_2 = nn.Linear(400, 300)\n",
    "        self.layer_3 = nn.Linear(300, 1)\n",
    "\n",
    "    def forward(self, x, u):\n",
    "        xu = torch.cat([x, u], 1)\n",
    "        print(xu.size())\n",
    "        print(x.size(), u.size())\n",
    "        x1 = F.relu(self.layer_1(xu))\n",
    "        x1 = F.relu(self.layer_2(x1))\n",
    "        x1 = self.layer_3(x1)\n",
    "        return x1\n",
    "\n",
    "\n",
    "# Selecting the device (CPU or GPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "\n",
    "# Building the whole DDPG Training Process into a class\n",
    "class DDPG(object):\n",
    "\n",
    "    def __init__(self, state_dim, action_dim, max_action):\n",
    "        self.actor = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target = Actor(state_dim, action_dim, max_action).to(device)\n",
    "        self.actor_target.load_state_dict(self.actor.state_dict())\n",
    "        self.actor_optimizer = torch.optim.Adam(self.actor.parameters())\n",
    "        self.critic = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target = Critic(state_dim, action_dim).to(device)\n",
    "        self.critic_target.load_state_dict(self.critic.state_dict())\n",
    "        self.critic_optimizer = torch.optim.Adam(self.critic.parameters())\n",
    "        self.max_action = max_action\n",
    "\n",
    "    def select_action(self, state):\n",
    "        state = torch.Tensor(state.reshape(1, -1)).to(device)\n",
    "        return self.actor(state).cpu().data.numpy().flatten()\n",
    "\n",
    "        # Learning Process for the DDPG Algorithm\n",
    "\n",
    "    def train(self, replay_buffer, iterations, batch_size=100, discount=0.99, tau=0.005, policy_noise=0.2,\n",
    "              noise_clip=0.5):\n",
    "\n",
    "        for it in range(iterations):\n",
    "\n",
    "            # Step 4: We sample a batch of transitions (s, s', a, r) from the memory\n",
    "            batch_states, batch_next_states, batch_actions, batch_rewards = replay_buffer.sample(\n",
    "                batch_size)\n",
    "            state = torch.Tensor(batch_states).to(device)\n",
    "            next_state = torch.Tensor(batch_next_states).to(device)\n",
    "            action = torch.Tensor(batch_actions).to(device)\n",
    "            reward = torch.Tensor(batch_rewards).to(device)\n",
    "\n",
    "            # Step 5: From the next state s', the Actor target plays the next action a'\n",
    "            next_action = self.actor_target(next_state)\n",
    "            print(next_action.size())\n",
    "\n",
    "            # Step 6: We add Gaussian noise to this next action a' and we clamp it in a range of values supported\n",
    "            # by the environment\n",
    "            noise = torch.Tensor(batch_actions).data.normal_(0, policy_noise).to(device)\n",
    "            noise = noise.clamp(-noise_clip, noise_clip)\n",
    "            next_action = (next_action + noise).clamp(-self.max_action, self.max_action)\n",
    "\n",
    "            # Step 7: The Critic Target take (s', a') as input and return Q-value Qt(s', a') as output\n",
    "            target_q = self.critic_target(next_state, next_action)\n",
    "\n",
    "            # Step 8: We get the estimated reward, which is: r' = r + γ * Qt, where γ id the discount factor\n",
    "            target_q = reward + (discount * target_q).detach()\n",
    "\n",
    "            # Step 9: The Critic models take (s, a) as input and return Q-value Q(s, a) as output\n",
    "            current_q = self.critic(state, action)\n",
    "\n",
    "            # Step 10: We compute the loss coming from the Critic model: Critic Loss = MSE_Loss(Q(s,a), Qt)\n",
    "            critic_loss = F.mse_loss(current_q, target_q)\n",
    "\n",
    "            # Step 11: We back propagate this Critic loss and update the parameters of the Critic model with a SGD\n",
    "            # optimizer\n",
    "            self.critic_optimizer.zero_grad()\n",
    "            critic_loss.backward()\n",
    "            self.critic_optimizer.step()\n",
    "\n",
    "            # Step 12: We update our Actor model by performing gradient ascent on the output of the first Critic model\n",
    "            actor_loss = -self.critic.forward(state, self.actor(state)).mean()\n",
    "            self.actor_optimizer.zero_grad()\n",
    "            actor_loss.backward()\n",
    "            self.actor_optimizer.step()\n",
    "\n",
    "            # Step 13: We update the weights of the Actor target by polyak averaging\n",
    "            for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "            # Step 14: We update the weights of the Critic target by polyak averaging\n",
    "            for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):\n",
    "                target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)\n",
    "\n",
    "    # Making a save method to save a trained model\n",
    "    def save(self, filename, directory):\n",
    "        torch.save(self.actor.state_dict(), '%s/%s_actor.pth' % (directory, filename))\n",
    "        torch.save(self.critic.state_dict(), '%s/%s_critic.pth' % (directory, filename))\n",
    "\n",
    "    # Making a load method to load a pre-trained model\n",
    "    def load(self, filename, directory):\n",
    "        self.actor.load_state_dict(torch.load('%s/%s_actor.pth' % (directory, filename)))\n",
    "        self.critic.load_state_dict(torch.load('%s/%s_critic.pth' % (directory, filename)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We set the parameters\n",
    "save_models = True  # Boolean checker whether or not to save the pre-trained model\n",
    "expl_noise = 0.1  # Exploration noise - STD value of exploration Gaussian noise\n",
    "batch_size = 100  # Size of the batch\n",
    "discount = 0.99  # Discount factor gamma, used in the calculation of the total discounted reward\n",
    "tau = 0.005  # Target network update rate\n",
    "policy_noise = 0.2  # STD of Gaussian noise added to the actions for the exploration purposes\n",
    "noise_clip = 0.5  # Maximum value of the Gaussian noise added to the actions (policy)\n",
    "alpha = 0.5  # Adjusting co-efficient for the comfort reward\n",
    "beta = 1  # Adjusting co-efficient for the energy reward\n",
    "obs = np.array([20.0, 50.0, 20.0, 0.1, 5.5, 1.0, 1.0, 0.0001, 0.0001])  # Initial state of the trnsys env\n",
    "modelname = 'CELLS_v1'\n",
    "days = 90  # Number of days the simulation is run for\n",
    "hours = 24  # Number of hours each day the simulation is run for\n",
    "minutes = 60\n",
    "seconds = 60\n",
    "ep_timestep = 4  # Number of timesteps per hour\n",
    "numsteps = days * hours * ep_timestep\n",
    "timestop = days * hours * minutes * seconds\n",
    "secondstep = timestop / numsteps\n",
    "\n",
    "# We create a filename for the two saved models: the Actor and Critic Models\n",
    "file_name = \"%s_%s\" % (\"DDPG\", \"DIET\")\n",
    "print(\"---------------------------------------\")\n",
    "print(\"Settings: %s\" % file_name)\n",
    "print(\"---------------------------------------\")\n",
    "\n",
    "# We get the necessary information on the states and actions in the chosen environment\n",
    "state_dim = 6\n",
    "action_dim = 1\n",
    "max_action = 21\n",
    "\n",
    "# We create the policy network (the Actor model)\n",
    "policy = DDPG(state_dim, action_dim, max_action)\n",
    "\n",
    "# We create the Experience Replay memory\n",
    "replay_buffer = ReplayBuffer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function taken from CBE comfort tool to calculate the pmv value for comfort evaluation\n",
    "def comfPMV(ta, tr, rh, vel=0.1, met=1.1, clo=1, wme=0):\n",
    "    pa = rh * 10 * math.exp(16.6536 - 4030.183 / (ta + 235))\n",
    "\n",
    "    icl = 0.155 * clo  # thermal insulation of the clothing in M2K/W\n",
    "    m = met * 58.15  # metabolic rate in W/M2\n",
    "    w = wme * 58.15  # external work in W/M2\n",
    "    mw = m - w  # internal heat production in the human body\n",
    "    if icl <= 0.078:\n",
    "        fcl = 1 + (1.29 * icl)\n",
    "    else:\n",
    "        fcl = 1.05 + (0.645 * icl)\n",
    "\n",
    "    # heat transfer coefficient by forced convection\n",
    "    hcf = 12.1 * math.sqrt(vel)\n",
    "    taa = ta + 273\n",
    "    tra = tr + 273\n",
    "    # we have verified that using the equation below or this tcla = taa + (35.5 - ta) / (3.5 * (6.45 * icl + .1))\n",
    "    # does not affect the PMV value\n",
    "    tcla = taa + (35.5 - ta) / (3.5 * icl + 0.1)\n",
    "\n",
    "    p1 = icl * fcl\n",
    "    p2 = p1 * 3.96\n",
    "    p3 = p1 * 100\n",
    "    p4 = p1 * taa\n",
    "    p5 = (308.7 - 0.028 * mw) + (p2 * math.pow(tra / 100.0, 4))\n",
    "    xn = tcla / 100\n",
    "    xf = tcla / 50\n",
    "    eps = 0.00015\n",
    "\n",
    "    n = 0\n",
    "    while abs(xn - xf) > eps:\n",
    "        xf = (xf + xn) / 2\n",
    "        hcn = 2.38 * math.pow(abs(100.0 * xf - taa), 0.25)\n",
    "        if hcf > hcn:\n",
    "            hc = hcf\n",
    "        else:\n",
    "            hc = hcn\n",
    "        xn = (p5 + p4 * hc - p2 * math.pow(xf, 4)) / (100 + p3 * hc)\n",
    "        n += 1\n",
    "        if n > 150:\n",
    "            print('Max iterations exceeded')\n",
    "            return 1  # fixme should not return 1 but instead PMV=999 as per ashrae standard\n",
    "\n",
    "    tcl = 100 * xn - 273\n",
    "\n",
    "    # heat loss diff. through skin\n",
    "    hl1 = 3.05 * 0.001 * (5733 - (6.99 * mw) - pa)\n",
    "    # heat loss by sweating\n",
    "    if mw > 58.15:\n",
    "        hl2 = 0.42 * (mw - 58.15)\n",
    "    else:\n",
    "        hl2 = 0\n",
    "    # latent respiration heat loss\n",
    "    hl3 = 1.7 * 0.00001 * m * (5867 - pa)\n",
    "    # dry respiration heat loss\n",
    "    hl4 = 0.0014 * m * (34 - ta)\n",
    "    # heat loss by radiation\n",
    "    hl5 = 3.96 * fcl * (math.pow(xn, 4) - math.pow(tra / 100.0, 4))\n",
    "    # heat loss by convection\n",
    "    hl6 = fcl * hc * (tcl - ta)\n",
    "\n",
    "    ts = 0.303 * math.exp(-0.036 * m) + 0.028\n",
    "    pmv = ts * (mw - hl1 - hl2 - hl3 - hl4 - hl5 - hl6)\n",
    "    ppd = 100.0 - 95.0 * math.exp(-0.03353 * pow(pmv, 4.0) - 0.2179 * pow(pmv, 2.0))\n",
    "\n",
    "    return pmv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r'C:\\Users\\achatter\\Desktop\\PhDResearch\\DIET\\Controller\\Cosimulation_energyplus')\n",
    "model = load_fmu(modelname+'.fmu')\n",
    "opts = model.simulate_options()  # Get the default options\n",
    "opts['ncp'] = numsteps  # Specifies the number of timesteps\n",
    "opts['initialize'] = False\n",
    "simtime = 0\n",
    "model.initialize(simtime, timestop)\n",
    "index = 0\n",
    "t = np.linspace(0.0, timestop, numsteps)/ minutes * seconds\n",
    "inputcheck_heating = np.zeros((numsteps, 1))\n",
    "tair = np.zeros((numsteps, 1))\n",
    "rh = np.zeros((numsteps, 1))\n",
    "tmrt = np.zeros((numsteps, 1))\n",
    "tout = np.zeros((numsteps, 1))\n",
    "occ = np.zeros((numsteps, 1))\n",
    "qheat = np.zeros((numsteps, 1))\n",
    "pmv = np.zeros((numsteps, 1))\n",
    "reward = np.zeros((numsteps, 1))\n",
    "action = np.zeros((numsteps, 1))\n",
    "state = np.zeros((numsteps, 6))\n",
    "\n",
    "for sim_num in range(5):\n",
    "\n",
    "    if sim_num != 0:\n",
    "        print(\"Total timesteps: {} Episode Num: {} Reward: {}\".format(numsteps, sim_num, np.sum(reward.flatten())))\n",
    "        policy.train(replay_buffer, (numsteps - 1) * sim_num, batch_size, discount, tau, policy_noise, noise_clip)\n",
    "\n",
    "        if sim_num > 1:\n",
    "            policy.save(file_name, directory=\"./pytorch_models/01032022\")  # Change the folder name here\n",
    "\n",
    "    while simtime < timestop:\n",
    "\n",
    "        if sim_num < 2:\n",
    "            action[index] = round(random.uniform(12, 21), 1)  # Choosing random values between 12 and 24 deg\n",
    "\n",
    "        else:  # After 1 episode, we switch to the model\n",
    "            action_arr = policy.select_action(obs)\n",
    "            # If the explore_noise parameter is not 0, we add noise to the action and we clip it\n",
    "            if expl_noise != 0:\n",
    "                action_arr = (action_arr + np.random.normal(0, expl_noise, size=1)).clip(12.0, 21.0)\n",
    "                action[index] = action_arr[0]\n",
    "\n",
    "        model.set('Thsetpoint_diet', action[index])\n",
    "        res = model.do_step(current_t=simtime, step_size=secondstep, new_step=True)        \n",
    "        tair[index], rh[index], tmrt[index], tout[index], qheat[index], occ[index], inputcheck_heating[index] = model.get(['Tair', 'RH', 'Tmrt', 'Tout', 'Qheat', 'Occ', 'Thsetpoint_diet'])\n",
    "        state[index][0], state[index][1], state[index][2], state[index][3], state[index][4], state[index][5] = tair[index], rh[index], tmrt[index], tout[index], occ[index], qheat[index]\n",
    "        pmv[index] = comfPMV(tair[index], tmrt[index], rh[index])\n",
    "        reward[index] = beta * (1 - (qheat[index] / 650)) + alpha * (1 - abs(pmv[index] + 0.5)) * int(bool(occ[index]))\n",
    "\n",
    "        if index == 0:\n",
    "            obs = np.array([tair[index], rh[index], tmrt[index], tout[index], occ[index], qheat[index]]).flatten()\n",
    "\n",
    "        else:\n",
    "            new_obs = np.array([tair[index], rh[index], tmrt[index], tout[index], occ[index], qheat[index]]).flatten()\n",
    "            # We store the new transition into the Experience Replay memory (ReplayBuffer)\n",
    "            replay_buffer.add((obs, new_obs, action[index], reward[index]))\n",
    "            obs = new_obs\n",
    "\n",
    "        simtime += secondstep\n",
    "        index += 1\n",
    "\n",
    "    # Writing to .csv files to save the data from the episode\n",
    "    np.savetxt(\"./Training_Data/01032022/Ep\"+str(sim_num+1)+\"/state.csv\", state[:-1, :], delimiter=\",\")\n",
    "    np.savetxt(\"./Training_Data/01032022/Ep\"+str(sim_num+1)+\"/next_state.csv\", state[1:, :], delimiter=\",\")\n",
    "    np.savetxt(\"./Training_Data/01032022/Ep\"+str(sim_num+1)+\"/action.csv\", action[1:, :], delimiter=\",\")\n",
    "    np.savetxt(\"./Training_Data/01032022/Ep\"+str(sim_num+1)+\"/reward.csv\", reward[1:, :], delimiter=\",\")\n",
    "    np.savetxt(\"./Training_Data/01032022/Ep\"+str(sim_num+1)+\"/pmv.csv\", pmv[1:, :], delimiter=\",\")\n",
    "\n",
    "    # Plotting the summary of simulation\n",
    "    fig = make_subplots(rows=6, cols=1, shared_xaxes=True, vertical_spacing=0.04, specs=[[{\"secondary_y\": False}], [{\"secondary_y\": False}], [{\"secondary_y\": False}],\n",
    "                                                                                         [{\"secondary_y\": True}], [{\"secondary_y\": True}], [{\"secondary_y\": False}]])\n",
    "\n",
    "    # Add traces\n",
    "    fig.add_trace(go.Scatter(name='Tair(state)', x=t, y=tair.flatten(), mode='lines', line=dict(width=1, color='cyan')), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(name='Tair_avg', x=t, y=pd.Series(tair.flatten()).rolling(window=24).mean(), mode='lines', line=dict(width=2, color='blue')), row=1, col=1)\n",
    "    fig.add_trace(go.Scatter(name='Tset(action)', x=t, y=action.flatten(), mode='lines', line=dict(width=1, color='fuchsia')), row=2, col=1)\n",
    "    fig.add_trace(go.Scatter(name='Tset_avg', x=t, y=pd.Series(action.flatten()).rolling(window=24).mean(), mode='lines', line=dict(width=2, color='purple')), row=2, col=1)\n",
    "    fig.add_trace(go.Scatter(name='Pmv', x=t, y=pmv.flatten(), mode='lines', line=dict(width=1, color='gold')), row=3, col=1)\n",
    "    fig.add_trace(go.Scatter(name='Pmv_avg', x=t, y=pd.Series(pmv.flatten()).rolling(window=24).mean(), mode='lines', line=dict(width=2, color='darkorange')), row=3, col=1)\n",
    "    fig.add_trace(go.Scatter(name='Heating', x=t, y=qheat.flatten(), mode='lines', line=dict(width=1, color='red')), row=4, col=1, secondary_y=False)\n",
    "    fig.add_trace(go.Scatter(name='Heating_cumulative', x=t, y=np.cumsum(qheat.flatten()), mode='lines', line=dict(width=2, color='darkred')), row=4, col=1, secondary_y=True)\n",
    "    fig.add_trace(go.Scatter(name='Reward', x=t, y=reward.flatten(), mode='lines', line=dict(width=1, color='lime')), row=5, col=1, secondary_y=False)\n",
    "    fig.add_trace(go.Scatter(name='Reward_cum', x=t, y=np.cumsum(reward.flatten()), mode='lines', line=dict(width=2, color='darkgreen')), row=5, col=1, secondary_y=True)\n",
    "    fig.add_trace(go.Scatter(name='Occupancy', x=t, y=occ.flatten(), mode='lines', line=dict(width=1, color='black')), row=6, col=1)\n",
    "\n",
    "    # Set x-axis title\n",
    "    fig.update_xaxes(title_text=\"Timestep (-)\", row=6, col=1)\n",
    "    fig.update_xaxes(nticks=50)\n",
    "\n",
    "    # Set y-axes titles\n",
    "    fig.update_yaxes(title_text=\"<b>Tair</b> (°C)\", range=[10, 24], row=1, col=1)\n",
    "    fig.update_yaxes(title_text=\"<b>Tset</b> (°C)\", range=[12, 21], row=2, col=1)\n",
    "    fig.update_yaxes(title_text=\"<b>PMV</b> (-)\", row=3, col=1)\n",
    "    fig.update_yaxes(title_text=\"<b>Heat Power</b> (kJ/hr)\", row=4, col=1, secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"<b>Heat Energy</b> (kJ)\", row=4, col=1, secondary_y=True)\n",
    "    fig.update_yaxes(title_text=\"<b>Reward</b> (-)\", row=5, col=1, range=[-3, 3], secondary_y=False)\n",
    "    fig.update_yaxes(title_text=\"<b>Tot Reward</b> (-)\", row=5, col=1, secondary_y=True)\n",
    "    fig.update_yaxes(title_text=\"<b>Fraction</b> (-)\", row=6, col=1)    \n",
    "\n",
    "    fig.update_layout(template='plotly_white', font=dict(family=\"Courier New, monospace\", size=12), legend=dict(orientation=\"h\", yanchor=\"bottom\", y=1, xanchor=\"right\", x=1))\n",
    "    pyo.plot(fig, filename=\"./Training_Data/01032022/Ep\"+str(sim_num+1)+\"/results.html\")\n",
    "\n",
    "policy.save(file_name, directory=\"./pytorch_models/01032022\")  # Change the folder name here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sum(qheat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "replay_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python37464bitbasecondafa8f368d65524f94bdb33a0d9561a17f"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
